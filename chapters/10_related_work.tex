\documentclass[../main]{subfiles}
\begin{document}


\chapter{Related work}
\label{ch:related_work}
Clustering mutants has been done before.
The research that exists tries to cluster mutants by defining the centroid of a cluster and cluster mutants on that definition.
Our research extends mutation clustering by defining a centroid that is dynamic.
In other words the centroids of our clusters are defined by the mutants itself.

\section{Clustering overlapped mutants}
\label{ch:overlapping_mutants}
Ma et al., clustered mutants by overlap\cite{Ma2016}.
They defined the term overlap as as mutants that are functionally equivalent.
This is close to an equivalent mutant but not the same.
They explain that an equivalent mutants is functionally identical to the original source code while an overlapped mutant is functionally identical to at least one other mutant.
If a mutant does not have an overlapped mutant that mutant would be a cluster on its own.
The achieved a reduction, in mutants executed, of 10\%.
They write in their research that it is limited to the mutator operators on expression level. 
If a mutator did not generate two or more mutants they could not detect overlapping mutants.
Their future work includes adding more mutators and widening the scope level to statements or blocks.
\newline
The research does not include efficiency.
While they show that they can cluster mutants based on overlap they do not show how efficient the mutation testing process is compared to executing a full set of mutants.
A threat to their validity, which is not addressed, is the fact that two overlapping mutants may not be challenged by the same tests and thus can result in different results.
Our research solves this problem by taking into account the location of the mutants.
Chapter \ref{ch:characteristics_location} describes what characteristics we use.

\section{Clustering by scope}
Yu et al., extended the research about overlapping mutants(see chapter \ref{ch:overlapping_mutants})\cite{Yu2019PossibilityScope}.
They extended the scope by adding more mutators.
The also extended the research to include statements and blocks.
Their results shows an increase in clustered mutants.
The research of Yu et al., suffers from the same limitations and threats  mentioned in Chapter \ref{ch:overlapping_mutants}.

\section{Clustering Hamming distance}
Ji et al., did a qualitative domain analysis on mutants\cite{Ji2009}.
As a result they identified the Hamming distance to cluster with. 
The Hamming distance is, like the Levenshtein distance, a similarity measure.
They use the k-means algorithm to cluster mutants represented by Hamming distance.
They write that the reduced test set in their experiment is still as strong as the original test set\cite{Ji2009}.
\newline
Ji et al., were successful in clustering mutants with Hamming distance and the k-means algorithm.
They do acknowledge that their research still has problems with rationally determining the domains of their clusters.
Our research does not have this problem, as the centre of the domains are decided by the characteristics of the mutants.

\section{Spectral clustering}
Wei et al., makes use of an intelligent technique, namely spectral clustering, to improve the efficacy of mutant reduction\cite{Wei2021SpectralTesting}.
They defined multiple definitions for the mutants; similarity distance, distance between the mutants and a killing matrix of the mutants.
With these definitions they reduced the mutants based on their proposed reduction method.
This method is based on spectral clustering (SCMT), the determination method of the number of clusters, spectral clustering of mutants, and selection of representative mutants.
The reduced mutants were then clustered with a classical clustering algorithm.
Their results are promising and show high cluster accuracy.
They write that there is still work left to do in optimizing the matrices and clustering algorithms.
Our goal overlaps with that of Wei, the difference is that we used a different methodology to achieve that goal.
Another difference is that our research does not require to know the result of a mutant(killed or survived).

\section{Clustering similarity}
Hussain et al., used the k-means and agglomerative clustering algorithm to cluster mutants according to a similarity measure. 
They used the Hamming distance as similarity measure.
They calculated the distance and used this as data to feed into the clustering algorithms.
The number of clusters and the initial position of the cluster center in the k-means algorithm are difficult to determine, and the process of the agglomerative clustering algorithm makes it difficult to correct the existing cluster formation\cite{Hussain2008}.
They did take into account efficiency.
Hussain et al, had the same goals as our research. 
The difference is that our research used different and more elaborated methods to cluster mutants.
Hussain et al., calculated the mutation score by counting one cluster as one mutant.
Our research calculated a weighted mutation score to reflect the clusters more accurately.

\section{Generalizing mutants}
Wilinski et al., tried to generalize the mutants by defining metrics.
Each metric is calculated for a specific mutator.
The three metrics are usefulness, frequency and dependency.
Combining the results of usefulness and frequency metrics, they observed that reducing the number of generated mutants gives noticeable cost reduction without a loss of the mutation score accuracy.
Wilinski et al., their research is narrowly scoped to the specific mutators they decided to do research on.
Our research does not contain this limitation.
The mutator is used as a characteristic in our research instead of limiting our research it helps define a mutant.

\end{document}