\documentclass[../main]{subfiles}
\begin{document}

\chapter{Background}
\label{ch:background}
This chapter presents the necessary background information for this thesis. First, we define some basic terminology that will be used throughout this thesis.
This thesis relies on clustering techniques.
We review the state of the art on clustering algorithms and secondly the state of the art on machine learning models specialized in clustering.

\section{Terminology}
\textbf{Mutation testing}
a form of white box testing in which testers change specific components of an application's source code to ensure a software test suite will be able to detect the changes.
\textbf{Levenshtein distance} the minimum number of single-character edits (insertions, deletions or substitutions) required to change one piece of text into the other.
\textbf{Maven} build tooling for Java software projects.
\textbf{Gradle} build tooling for Java and Android software projects.
\textbf{Mutator} definition of how the original source code is transformed into a mutant.
\textbf{test engine} a test engine facilitates discovery and execution of tests. Examples for Java are TestNG and JUnit.
\textbf{weighted mutation score} mutation score calculation designed to be comparable with the mutation score of a full set(see Chapter \ref{ch:weighted_score} for a detailed explanation).

\section{Clustering algorithms}
This research focuses on clustering mutants and use of existing clustering algorithms. 
Many different types of clustering algorithms have been proposed\cite{Rodriguez2019}. While there is a lot of diversity, some methods are more frequently used than others\cite{Wu2008TopMining}. 
Rodriguez et al., compared the performance of nine different clustering algorithms:
\begin{itemize}
    \item k-means
    \item CLARA
    \item hierarchical
    \item EM
    \item hcmodel
    \item spectral
    \item subscpace
    \item optics
    \item dbscan
\end{itemize}
The goal of their study was to guide researchers, who have little experience in data mining techniques, to the application of clustering methods.
They evaluated the algorithms in three distinct situations: default parameters, single parameter variation and random variation of parameters.
They used 400 generated artificial data sets which were normally distributed.
\newline
The results reported in their research are respective to specific configurations of normally distributed data and algorithmic implementations.
Nonetheless they do give a good overview on how the algorithms compare to each other.
\newline
For the default parameter experiment, the spectral clustering algorithm had the best performance and the hierarchical algorithm had the worst performance.
\newline
Regarding single parameter variations, for data sets containing 2 columns, the hierarchical, optics and EM methods showed significant performance variation.
\newline
With respect to the multidimensional analysis for data sets, the performance of the algorithms for the multidimensional selection of parameters was similar to that using the default parameters.
They conclude their research with observing that, for data sets with 10 or more columns the spectral algorithm consistently provided the best performance.
However the EM, hierarchical, k- means and subspace algorithms can also achieve similar performance with some parameter tuning.
The optics and dbscan algorithms aim at different data distributions than performed in this study. There different results could be obtained for non-normally distributed data.

\section{Machine learning models for clustering}
\label{ch:neuralNetworkSurvey}
There are many different types of machine learning models with their respective applications.
For this thesis we will focus on machine learning models designed for clustering.
\newline
Machine learning algorithms are divided in supervised and unsupervised learning algorithms\cite{supervisedUnsupervised}.
Supervised learning is the machine learning task of learning a function that maps an input to an output based on example input-output pairs\cite{supervisedUnsupervised}.
In contrast to supervised learning, unsupervised learning shows self-organization that captures patterns as neuronal predilections or probability densities\cite{supervisedUnsupervised}.
Unsupervised learning algorithms are commonly used for classification and categorization\cite{supervisedUnsupervised}.
Unsupervised learning is a type of algorithm that learns patterns from untagged data\cite{supervisedUnsupervised}.
These machine learning algorithms are commonly used for clustering and dimensionality reduction\cite{supervisedUnsupervised}.
\newline
K.L. Du surveyed different machine learning models used for clustering\cite{Du2010Clustering:Approach}.
Relevant models he discusses are:
\begin{itemize}
    \item Self organizing map.
    \item Learning vector quantization.
    \item C-means(K-means) clustering.
    \item Mountain and subtractive clustering.
    \item Neural gas.
    \item ART networks.
    \item Fuzzy c-means.
\end{itemize}
K.L. Du describes the mathematical principles on which the machine learning algorithms are based as well as the origin, pros and cons of each model.
\newline
He also touches on subjects relevant to clustering and machine learning models such as the under-utilization problem, fuzzy clustering, robust clustering, clustering based on non- Euclidean distance measures, supervised clustering, and hierarchical clustering.
Machine learning model variants and their references are also discussed.
\newline
He closes his paper with computer simulations of some of the machine learning models he surveyed.

\end{document}