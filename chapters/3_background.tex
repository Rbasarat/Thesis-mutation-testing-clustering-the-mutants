\documentclass[../main]{subfiles}
\begin{document}

\chapter{Background}
\label{ch:background}
This chapter presents the necessary background information for this thesis. First, we define some basic terminology that will be used throughout this thesis.
Next, we will compare existing mutation testing tools. We start of by reviewing existing literature and then extend the comparison. 
This thesis relies on clustering techniques.
We review the state of the art on clustering algorithms and secondly the state of the art on machine learning models specialized in clustering.

\section{Terminology}
Subsuming mutants.
Mutation testing.
Levenshtein distance.
Maven.
Gradle.
Mutator.
test engine.
weighted mutation score.
\section{Project selection}
\label{ch:project_selection}
We choose three main requirements for selecting software projects; the projects should have a test suite, the test suite should not contain failing tests and the mutation testing tool should be able to execute mutants for the sample project.
We selected projects that were also used in other research within the mutation testing and testing domain\cite{Pizzoleto2019,Yu2019PossibilityScope,Wei2021SpectralTesting, Zhang2019PredictiveTesting, Chen2018SpeedingStudy, Laurent2017AssessingPIT}.
\newline
We extend our sample by selecting projects from the first six pages of the most popular Java projects on GitHub\footnote{\url{https://github.com/search?p=7&q=language\%3Ajava+stars\%3A\%3E10000&type=Repositories}}.
The unfiltered sample contains 50+ projects.
From there we filter out all the projects that contain failing tests and that are not libraries or applications.
The result consists of a sample with fifteen projects in total.


\section{Clustering algorithms}
This research focuses on clustering mutants and use of existing clustering algorithms. 
Many different types of clustering algorithms have been proposed\cite{Rodriguez2019}. While there is a lot of diversity, some methods are more frequently used than others\cite{Wu2008TopMining}. 
Rodriguez et al., compared the performance of nine different clustering algorithms:
\begin{itemize}
    \item k-means
    \item CLARA
    \item hierarchical
    \item EM
    \item hcmodel
    \item spectral
    \item subscpace
    \item optics
    \item dbscan
\end{itemize}
The goal of their study was to guide researchers, who have little experience in data mining techniques, to the application of clustering methods.
They evaluated the algorithms in three distinct situations: default parameters, single parameter variation and random variation of parameters.
They used 400 generated artificial data sets which were normally distributed.
\newline
The results reported in their research are respective to specific configurations of normally distributed data and algorithmic implementations.
Nonetheless they do give a good overview on how the algorithms compare to each other.
\newline
For the default parameter experiment, the spectral clustering algorithm had the best performance and the hierarchical algorithm had the worst performance.
\newline
Regarding single parameter variations, for data sets containing 2 columns, the hierarchical, optics and EM methods showed significant performance variation.
\newline
With respect to the multidimensional analysis for data sets, the performance of the algorithms for the multidimensional selection of parameters was similar to that using the default parameters.
They conclude their research with observing that, for data sets with 10 or more columns the spectral algorithm consistently provided the best performance.
However the EM, hierarchical, k- means and subspace algorithms can also achieve similar performance with some parameter tuning.
The optics and dbscan algorithms aim at different data distributions than performed in this study. There different results could be obtained for non-normally distributed data.

\section{Machine learning models for clustering}
\label{ch:neuralNetworkSurvey}
There are many different types of machine learning models with their respective applications.
For this thesis we will focus on machine learning models designed for clustering.
\newline
Machine learning algorithms are divided in supervised and unsupervised learning algorithms\cite{supervisedUnsupervised}.
Supervised learning is the machine learning task of learning a function that maps an input to an output based on example input-output pairs\cite{supervisedUnsupervised}.
In contrast to supervised learning, unsupervised learning shows self-organization that captures patterns as neuronal predilections or probability densities\cite{supervisedUnsupervised}.
Unsupervised learning algorithms are commonly used for classification and categorization\cite{supervisedUnsupervised}.
Unsupervised learning is a type of algorithm that learns patterns from untagged data\cite{supervisedUnsupervised}.
These machine learning algorithms are commonly used for clustering and dimensionality reduction\cite{supervisedUnsupervised}.
\newline
K.L. Du surveyed different machine learning models used for clustering\cite{Du2010Clustering:Approach}.
Relevant models he discusses are:
\begin{itemize}
    \item Self organizing map.
    \item Learning vector quantization.
    \item C-means(K-means) clustering.
    \item Mountain and subtractive clustering.
    \item Neural gas.
    \item ART networks.
    \item Fuzzy c-means.
\end{itemize}
K.L. Du describes the mathematical principles on which the machine learning algorithms are based as well as the origin, pros and cons of each model.
\newline
He also touches on subjects relevant to clustering and machine learning models such as the under-utilization problem, fuzzy clustering, robust clustering, clustering based on non- Euclidean distance measures, supervised clustering, and hierarchical clustering.
Machine learning model variants and their references are also discussed.
\newline
He closes his paper with computer simulations of some of the machine learning models he surveyed.

\end{document}