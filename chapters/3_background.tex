\documentclass[../main]{subfiles}
\begin{document}

\chapter{Background}
\label{ch:background}
This chapter will present the necessary background information for this thesis. First, we define some basic terminology that will be used throughout this thesis.
Next, we will compare existing mutation testing tools. We start of by reviewing existing literature and then extend the comparison. \todo{extend this later on?}
\section{Terminology}
\todo{do this when we finished experiments and all}
\todo{subsuming mutants}
\todo{levensteihn distance}
\todo{maven and gradle}
\section{Project selection}
We choose three main requirements for selecting software projects; the projects should have a test suite, the test suite should not contain failing tests and the mutation testing tool should be able to execute mutants for the sample project.
We selected projects that were also used in other research within the mutation testing and testing domain\cite{Pizzoleto2019,Yu2019PossibilityScope,Wei2021SpectralTesting, Zhang2019PredictiveTesting, Chen2018SpeedingStudy, Laurent2017AssessingPIT}.
We extend our sample by selecting projects from the first six pages of the most popular Java projects on GitHub\footnote{\url{https://github.com/search?p=7&q=language\%3Ajava+stars\%3A\%3E10000&type=Repositories}}.
The unfiltered sample contains 50+ projects.
From there we filter out all the projects that contain failing tests and that aren't libraries or applications.
The result consists of a sample with sixteen projects in total.


\section{Clustering algorithms}
This research focuses on clustering mutants and use of existing clustering algorithms. 
Many different types of clustering algorithms have been proposed\cite{Rodriguez2019}. While there is a lot of diversity, some methods are more frequently used than others\cite{Wu2008TopMining}. 
Rodriguez et al., compared the performance of nine different clustering algorithms:
\begin{itemize}
    \item k-means
    \item CLARA
    \item hierarchical
    \item EM
    \item hcmodel
    \item spectral
    \item subscpace
    \item optics
    \item dbscan
\end{itemize}
The goal of their study was to guide researchers, who have little experience in data mining techniques, to the application of clustering methods.
They evaluated the algorithms in three distinct situations: default parameters, single parameter variation and random variation of parameters.
They used 400 generated artificial data sets which were normally distributed.
\newline
The results reported in their research are respective to specific configurations of normally distributed data and algorithmic implementations.
Nonetheless they do give a good overview on how the algorithms compare to each other.
\newline
For the default parameter situation the spectral clustering algorithm had the best performance and the hierarchical algorithm had the worst performance.
\newline
Regarding single parameter variations, for data sets containing 2 columns, the hierarchical, optics and EM methods showed significant performance variation.
\newline
With respect to the multidimensional analysis for data sets, the performance of the algorithms for the multidimensional selection of parameters was similar to that using the default parameters.
They conclude their research with observing that, for data sets with 10 or more columns the spectral algorithm consistently provided the best performance.
However the EM, hierarchical, k- means and subspace algorithms can also achieve similar performance with some parameter tuning.
The optics and dbscan algorithms aim at different data distributions than performed in this study. There different results could be obtained for non-normally distributed data.

\section{Neural networks for clustering}
\label{ch:neuralNetworkSurvey}
There are many different types of neural networks.
These neural networks have different applications based on their properties.
For this thesis we will focus on neural networks designed for clustering.
\newline
Neural networks consists of machine learning algorithms\cite{supervisedUnsupervised}. 
These machine learning algorithms are divided in supervised and unsupervised learning algorithms\cite{supervisedUnsupervised}.
Supervised machine learning algorithms are commonly used for classification and categorization\cite{supervisedUnsupervised}.
Unsupervised machine learning algorithms are commonly used for clustering and dimensionality reduction\cite{supervisedUnsupervised}.
\newline
K.L. Du surveyed different neural networks and algorithms used for clustering\cite{Du2010Clustering:Approach}:
\begin{itemize}
    \item Self organizing map.
    \item Learning vector quantization.
    \item C-means clustering.
    \item Mountain clustering.
    \item Neural gas.
    \item ART networks.
    \item Fuzzy c-means.
\end{itemize}
In his paper he also touches on subjects relevant to clustering and neural networks such as the under-utilization problem, fuzzy clustering, robust clustering, clustering based on non- Euclidean distance measures, supervised clustering, and hierarchical clustering.
Neural network variants and their references are also discussed.
\newline
K.L. Du describes the mathematical principles on which the neural networks and algorithms are based.
He closes his paper with computer simulations of some of the neural networks he surveyed.

\end{document}