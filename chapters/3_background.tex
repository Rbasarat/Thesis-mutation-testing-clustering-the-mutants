\documentclass[../main]{subfiles}
\begin{document}

\chapter{Background}
\label{ch:background}
This chapter presents the necessary background information for this thesis. First, we define some basic terminology that will be used throughout this thesis.
This thesis relies on clustering techniques.
We review the state of the art on clustering algorithms and secondly the state of the art on machine learning models specialized in clustering.

\section{Terminology}
\textbf{Mutation testing}
a form of white box testing in which testers change specific components of an application's source code to ensure a software test suite will be able to detect the changes.
\textbf{Levenshtein distance} the minimum number of single-character edits (insertions, deletions or substitutions) required to change one piece of text into the other.
\textbf{Maven} build tooling for Java software projects.
\textbf{Gradle} build tooling for Java and Android software projects.
\textbf{Mutator} definition of how the original source code is transformed into a mutant.
\textbf{test engine} a test engine facilitates discovery and execution of tests. Examples for Java are TestNG and JUnit.
\textbf{weighted mutation score} mutation score calculation designed to be comparable with the mutation score of a full set(see Chapter \ref{ch:weighted_score} for a detailed explanation).

\section{Statistics}
The terminology of statistics may lead to confusion if not understood correctly.
Thus we discuss some basic terminology and principles in statistics.
When performing a statistical test we have two hypotheses; a null hypothesis and an alternative hypothesis.
A test contains a specific null and alternative hypothesis.
The goal of such a test is to reject the null hypothesis.
Rejecting a null hypothesis can be done by looking at the resulting \textit{p-value}.
The \textit{p-value} is the chance that the value of the statistical test occurs if the null hypothesis is true on a
zero to one scale.
We can reject a null hypothesis if we think that this chance is below a certain threshold.
This threshold is determined beforehand and named the \textit{alpha-value}.
A commonly used value for alpha is 5\%.
We cannot reject the null hypothesis if the \textit{p-value} is below the \textit{alpha-value}.
This does not necessarily mean that the null hypothesis is true.
This can have different reasons, for example, the data set used to validate the hypothesis is too small.

\section{Clustering algorithms}
This research focuses on clustering mutants and use of existing clustering algorithms. 
Many different types of clustering algorithms have been proposed\cite{Rodriguez2019}. While there is a lot of diversity, some methods are more frequently used than others\cite{Wu2008TopMining}. 
Rodriguez et al., compared the performance of nine different clustering algorithms.
We provide a summary of these algorithms in the following paragraphs.

\subsubsection{K-means} 
The K-means algorithm identifies \textit{k} number of centroids, and then allocates every data point to the nearest cluster, while keeping the centroids as small as possible\cite{Hartigan1979AlgorithmAlgorithm}.

\subsubsection{CLARA} 
CLARA uses multiple fixed samples of the data set to minimize sampling bias and, select the best medoids among those samples\cite{Sarle1991FindingAnalysis.}.
A medoid is defined as the object \textit{i} for which the average dissimilarity to all other objects in its cluster is minimal\cite{Sarle1991FindingAnalysis.}.
\subsubsection{Hierarchical clustering} 
Hierarchical clustering groups similar objects into groups called clusters\cite{Fraley1998HowAnalysis}. 
The endpoint is a set of clusters or groups, where each cluster is distinct from each other cluster, and the objects within each cluster are broadly similar to each other\cite{Fraley1998HowAnalysis}.
\subsubsection{EM} 
EM(expectation maximization) clustering technique is similar to the K-Means technique\cite{Fv1996TheAlgorithm}.
Instead of assigning examples to clusters to maximize the differences in means for continuous variables, the EM clustering algorithm computes probabilities of cluster memberships based on one or more probability distributions\cite{Fv1996TheAlgorithm}. 
\subsubsection{Spectral clustering}
Spectral clustering is a technique with roots in graph theory.
It identifies communities of nodes in a graph based on the edges connecting them\cite{Filippone2008AClustering}.
Spectral clustering uses information from the eigenvalues of special matrices built from the graph or the data set  to perform dimensionality reduction before clustering in fewer dimensions\cite{Filippone2008AClustering}. 
The similarity matrix is provided as an input and consists of a quantitative assessment of the relative similarity of each pair of points in the data set\cite{Filippone2008AClustering}.
\subsubsection{Subspace}
Subspace clustering algorithms consider the similarity between objects with respect to distinct subsets of the attributes\cite{Hartigan1979AlgorithmAlgorithm}. 
Different subsets of the attributes might define distinctions between each other. 
The algorithm can identify clusters that exist in multiple, possibly overlapping, sub spaces\cite{M2016SUBSPACEDATA}.
\subsubsection{Dbscan}
Dbscan is a density-based clustering non-parametric algorithm\cite{Daszykowski2009}.
Given a set of points in some space, it groups together points that are closely packed together, marking outliers as points that lie alone in low-density regions\cite{Daszykowski2009}.
\subsubsection{OPTICS}
The OPTICS(Ordering Points To Identify the Clustering Structure) algorithm starts with a data point and expands its neighborhood using a similar procedure as in the Dbscan algorithm\cite{Ankerst1999OPTICS:Structure}.
The difference is that the neighborhood is first expanded to points with low core-distance\cite{Ankerst1999OPTICS:Structure}. 
The optics algorithm can detect clusters having large density variations and irregular shapes\cite{Ankerst1999OPTICS:Structure}.
\newline 

The goal of their study was to guide researchers, who have little experience in data mining techniques, to the application of clustering methods.
They evaluated the algorithms in three distinct situations: default parameters, single parameter variation and random variation of parameters.
They used 400 generated artificial data sets which were normally distributed.
\newline
The results reported in their research are respective to specific configurations of normally distributed data and algorithmic implementations.
Nonetheless they do give a good overview on how the algorithms compare to each other.
\newline
For the default parameter experiment, the spectral clustering algorithm had the best performance and the hierarchical algorithm had the worst performance.
\newline
Regarding single parameter variations, for data sets containing 2 columns, the hierarchical, optics and EM methods showed significant performance variation.
\newline
With respect to the multidimensional analysis for data sets, the performance of the algorithms for the multidimensional selection of parameters was similar to that using the default parameters.
They conclude their research with observing that, for data sets with 10 or more columns the spectral algorithm consistently provided the best performance.
However the EM, hierarchical, k- means and subspace algorithms can also achieve similar performance with some parameter tuning.
The optics and dbscan algorithms aim at different data distributions than performed in this study. There different results could be obtained for non-normally distributed data.

\section{Machine learning models for clustering}
\label{ch:neuralNetworkSurvey}
There are many different types of machine learning models with their respective applications.
For this thesis we will focus on machine learning models designed for clustering.
\newline
Machine learning algorithms are divided in supervised and unsupervised learning algorithms\cite{supervisedUnsupervised}.
Supervised learning is the machine learning task of learning a function that maps an input to an output based on example input-output pairs\cite{supervisedUnsupervised}.
In contrast to supervised learning, unsupervised learning shows self-organization that captures patterns as neuronal predilections or probability densities\cite{supervisedUnsupervised}.
Unsupervised learning algorithms are commonly used for classification and categorization\cite{supervisedUnsupervised}.
Unsupervised learning is a type of algorithm that learns patterns from untagged data\cite{supervisedUnsupervised}.
These machine learning algorithms are commonly used for clustering and dimensionality reduction\cite{supervisedUnsupervised}.
\newline
K.L. Du surveyed different machine learning models used for clustering\cite{Du2010Clustering:Approach}.
We provide a summary of these models in the following paragraphs.

\subsubsection{Self organizing map.}
The \acrfull{som} is a neural network-based dimensionality reduction algorithm generally used to represent a high-dimensional data set as two-dimensional discretized pattern\cite{Du2010Clustering:Approach}.
Reduction in dimensionality is performed while retaining the topology of data present in the original feature space\cite{Du2010Clustering:Approach}.
The \acrlong{som} computes the Euclidean distance of the input pattern x to each neuron k, and finds the winning neuron using the nearest-neighbor rule\cite{Du2010Clustering:Approach} .
The winning node is called the excitation center.

\subsubsection{Fuzzy C-means clustering.}
Fuzzy C-means clustering works by assigning membership to each data point corresponding to each cluster center on the basis of distance between the cluster center and the data point\cite{Du2010Clustering:Approach}.
The more the data is near to the cluster center more its membership towards the particular cluster center.
The weighting parameter \textit{m} is called the fuzzifier\cite{Du2010Clustering:Approach}. 
\textit{M} determines the fuzziness of the partition produced, and reduces the influence of small membership values\cite{Du2010Clustering:Approach}.

\subsubsection{Learning vector quantization.}
One or more prototypes are used to represent each class in the data set, each prototype is described as a point in the feature space\cite{Du2010Clustering:Approach}. 
New (unknown) data points are then assigned the class of the prototype that is nearest to them\cite{Du2010Clustering:Approach}. 
In order for nearest to make sense, a distance measure has to be defined\cite{Du2010Clustering:Approach}. 
An example of such a distance measure is the Euclidean distance.
Learning vector quantization(\acrshort{lvq}) is a supervised classification model.
The \acrshort{lvq} can be treated as a supervised version of the \acrshort{som}\cite{Du2010Clustering:Approach}.

\subsubsection{Mountain and subtractive clustering.}
The mountain and subtractive clustering models are an effective method for estimating the number of clusters\cite{Du2010Clustering:Approach}. 
The subtractive clustering model is an extension of the mountain model\cite{Du2010Clustering:Approach}. 
\newline
The method grids the data space and computes a potential value for each grid point based on its distance to the actual data points. Each grid point is a potential cluster center. 
The potential for each grid is calculated based on the density of the surrounding data points. 
The grid with the highest potential is selected as the first cluster center and then the potential values of all the other grids are reduced according to their distances to the first cluster center\cite{Du2010Clustering:Approach}.

\subsubsection{Neural gas.}
The neural gas is a topology-preserving network, and can be treated as an extension to the C-means\cite{Du2010Clustering:Approach}.
A data optimal topological ordering is achieved by using neighborhood ranking within the input space at each training step\cite{Du2010Clustering:Approach}.
To find its neighborhood rank, each neuron compares its distance to the input vector with those of all the other neurons to the input vector\cite{Du2010Clustering:Approach}. 
Unlike the \acrlong{som}(see Chapter \ref{ch:topThreeModels}, which uses predefined static neighborhood relations, the neural gas network determines a dynamical neighborhood relation as learning proceeds\cite{Du2010Clustering:Approach}.
A benefit of the neural gas network is that it is able to spawn neurons.
This means that it is not necessary to know the number of clusters in advance.

\subsubsection{ART networks.}
ART models are characterized by systems of differential equations that formulate stable self-organizing learning methods\cite{Du2010Clustering:Approach}.
The stability and plasticity properties as well as the ability to efficiently process dynamic data make the ART attractive for clustering large, rapidly changing sequences of input patterns\cite{Du2010Clustering:Approach}.
\newline

K.L. Du describes the mathematical principles on which the machine learning algorithms are based as well as the origin, pros and cons of each model.
\newline
He also touches on subjects relevant to clustering and machine learning models such as the under-utilization problem, fuzzy clustering, robust clustering, clustering based on non- Euclidean distance measures, supervised clustering, and hierarchical clustering.
Machine learning model variants and their references are also discussed.
\newline
He closes his paper with computer simulations of some of the machine learning models he surveyed.

\end{document}