\documentclass[../main]{subfiles}
\begin{document}
\chapter{Experiment design}
In this chapter we design an experiment to validate our research.
We execute the following steps:
\begin{enumerate}
    \item Gather all mutant characteristics from PIT.
    \item Execute full set of mutants to gather baseline measurement.
    \item Cluster the mutants according the black and white box methodologies.
    \item Execute one random mutant per cluster and gather result.
    \item Calculate weighted mutation score.
\end{enumerate}
Both research questions have the same goal, but with different approaches.
We can devise one hypothesis for both research questions.
We hypothesise that we can cluster mutants with the set of characteristics we identified
while maintaining effectiveness and reducing the amount executed when executing one mutant from each cluster that is randomly selected.
Our goal is to achieve a weighted mutation score that is as close as possible to the mutation score of a full set of executed mutants.
We repeat the experiment for each approach we devised in Chapters \ref{ch:reserach_question1} and \ref{ch:reserach_question2}.

\section{Project selection}
\label{ch:project_selection}
We choose three main requirements for selecting software projects; the projects should have a test suite, the test suite should not contain failing tests and the mutation testing tool should be able to execute mutants for the sample project.
We selected projects that were also used in other research within the mutation testing and testing domain\cite{Pizzoleto2019,Yu2019PossibilityScope,Wei2021SpectralTesting, Zhang2019PredictiveTesting, Chen2018SpeedingStudy, Laurent2017AssessingPIT}.
\newline
We extend our sample by selecting projects from the first six pages of the most popular Java projects on GitHub\footnote{\url{https://github.com/search?p=7&q=language\%3Ajava+stars\%3A\%3E10000&type=Repositories}}.
The unfiltered sample contains 50+ projects.
From there we filter out all the projects that contain failing tests and that are not libraries or applications.
The result consists of a sample with fifteen projects in total.

\section{Baseline measurement}
The baseline for our experiment is the mutation score of the set of all mutants generated by the selected mutation testing tool.
To extract this score we need to execute a full set of mutants within PIT.
For each source we have added the PIT plugin and required configuration in the \textit{pom.xml}, this code instructs PIT to generate and execute mutants with all mutators available, an example of such code can be found in Appendix \ref{ap:PIT_maven_code}.
We can then use the \acrfull{cli} and a maven command(see Listing \ref{lst:mvn-full}) to execute PIT.
\begin{lstlisting}[label=lst:mvn-full,caption=Command used to execute full set of mutants with PIT.]
mvn -U org.pitest:pitest-maven:mutationCoverage
\end{lstlisting}

\section{Weighted mutation score}
\label{ch:weighted_score}
Our hypothesis states that each mutant executed should represent that whole cluster.
With a mutant executed from each cluster we can calculate a mutation score.
This mutation score is a weighted mutation score.
This weighted mutation score is the product of the result of a mutant(1 for killed and 0 for survived) and the amount of mutants in the cluster it represents.
The weighted mutation score is then comparable to the score of a full set as the total number of mutants will be the same.
\newline
For example, take a full set with a score of 75/100 killed mutants.
This gives us a mutation score of 75\%. 
We then cluster the mutants in four clusters consisting of 12, 30, 38 and 20 mutants, respectively.
We randomly select four mutants of each cluster and execute them.
The mutants representing cluster one and four survive and two and three are killed.
If we calculate the weighted score we get 68/100 which is 68\%.
We can then compare this to the score of a full set because the amount of mutants executed is the same: 75/100(75\%) and 68/100(68\%).

\section{Validation}
\label{ch:exp1_validation}
The most efficient way to measure test effectiveness with mutation testing is by executing all mutants that a mutation testing tool can possibly generate.
The goal of this research is to reduce the amount of mutants executed while maintaining effectiveness.
To reduce the amounts executed we cluster the mutants. 
We can measure this by counting the number of clusters we generate and compare it to the number of total mutants generated by the selected tool.
\newline
To validate how effective our method of clustering is we can compare the weighted mutation score(see Chapter \ref{ch:weighted_score}) of the clustered set to the mutation score of the full set.
The closer the weighted score is to that of a full set the more effective our set of characteristics and clustering algorithm proves to be.
In other words we want achieve a mutation score that is as close as possible as to that of a full set.
We select a statistical significance level of $\leq 0.05$.
This is the conventional threshold for declaring statistical significance\cite{Kirk1996PracticalCome}.
\newline
Depending on the effectiveness of our clustering algorithm we may loose accuracy.
This can happen if a cluster contains mutants of both results. 
We can measure the accuracy inside a cluster by calculating a percentage of all mutants that have survived against the ones that have been killed in a cluster.
If the majority of the mutants in a cluster is killed then we consider that cluster to represent a killed cluster and the other way around for survived mutants.
We consider the mutants that are not in the majority of the cluster as inaccuracy.
\newline
If the weighted mutation score of our clustered set deviates more than 5\% from the score of a full set we reject our hypothesis.

\section{Random mutant selection}
\label{ch:mutant_selection}
As stated in our hypothesis we randomly select a mutant from each cluster to be executed.
To validate our sample we make use of statistical hypothesis testing\cite{Emmert-Streib2019UnderstandingInference}. Our null hypothesis states that there is no relation between the characteristics identified and the results of a mutant with the chosen methodology.
To test this hypothesis we select an alpha value of 0.05.
To translate; if more than five percent of the values in a sample deviates more than the significance level(see Chapter \ref{ch:exp1_validation}) we cannot reject the null hypothesis.
To make our results reproducible we select the random mutant based on a generated seed.
The seed generated for each run will be included in the results and can be found in the Github repository in the file \textit{hierarchical-clustering/main.py} at line 182\cite{rbasarat-repo}.
We repeat our experiment 30 times with 30 different seeds.
Achieving consistent results while applying random selection contributes to the validity of the experiment.


\end{document}