\documentclass[../../main]{subfiles}
\begin{document}
\section{Clustering mutants}
\label{ch:clustering_characteristics}
We identified characteristics to represent mutants in Chapter \ref{ch:identifying_characteristics}.
With this representation of the mutants we can devise a methodology to cluster them.
The following sections describes the methodology for our white box approach of clustering mutants.

\subsection{Hierarchical clustering}
Clustering Levenshtein distances has been done before with hierarchical clustering with promising results.\cite{Rajalingam2011, Gothai2010PerformanceAlgorithms}. 
Research shows that hierarchical clustering performs better when clustering with at least ten features\cite{Rodriguez2019}.
It also states that varying the parameters of hierarchical clustering improves the performance compared to that of the default settings of the algorithm\cite{Rodriguez2019}.
We identified more than ten characteristics and can use them as features.
We adjust the parameters of the algorithm based on the characteristics in our data set.
Bases on these observations and the research of Rodriguez et al., we select hierarchical clustering as it gives us the best performance in terms of partitioning.
Next we explain the configuration we use for clustering the mutants. 
\newline
Hierarchical clustering is subdivided into agglomerative and divisive. 
The agglomerative hierarchical technique follows bottom up approach whereas divisive follows top-down approaches.
Hierarchical clustering uses different metrics which calculates the euclidean distance between two clusters and the linkage criteria\cite{Rajalingam2011}. 
The linkage criteria specifies the dissimilarity in the sets as a function of the pair-wise distances of observations in those sets\cite{Rajalingam2011}.
We reviewed the different linkage criteria and concluded the following.
\newline
Research shows that the complete linkage outperforms the single linkage method\cite{Vijaya2019ComparativeClustering}.
The ward linkage and complete linkage methods perform the same when clusters are well separated\cite{Vijaya2019ComparativeClustering}.
However if the clusters overlap the ward linkage outperforms the complete linkage\cite{Vijaya2019ComparativeClustering}.
\newline
We identify all characteristics per mutant as a separate cluster.
Starting out with each mutant as a separate cluster we can use the agglomerative form of hierarchical clustering.
Since we cannot assume that our clusters are well separated we chose to use the ward linkage method for our clustering algorithm.
In summary, we select the agglomerative hierarchical clustering algorithm with ward linkage to cluster mutants represented by the characteristics we gathered.

\subsection{Levenshtein distance on Java byte code}
A mutant is a piece of code that differs in a predefined way from its parent.
Java source code compiles into Java byte code.
During the compilation of Java code certain context is abstracted away\cite{byteCodeEngineering}.
Optimisations are applied which also changes the Java byte code\cite{byteCodeEngineering}.
As a result the textual similarity between a Java code mutant and its parent is different from the similarity between a Java byte code mutant and its parent.
While the textual similarity may be different between the Java code and Java byte code the functionality remains the same.
Java byte code reflects more of the semantic nature of the source code than the source code itself does. 
PIT generates and executes mutants on byte code level\cite{pitestBytecode}. 
In other words PIT executes the unit tests of a source against the byte code of a mutant.
\newline
By calculating the distance on byte code we filter out the context that is present in Java code.
This gives us a Levenshtein distance that represents more of the semantic difference, between a mutant and parent, than calculating the distance for Java code.
For this characteristic we use the Levenshtein distance between a Java byte code mutant and its parent.

\subsection{Levenshtein distance implementation}
There are different implementations of the Levensthein distance.
For our experiment we need to calculate the Levenshtein distance on the byte code generated in PIT.
Since we make use of the plugin system we also choose to use an implementation that is written in Java.
The Levenshtein distance can be calculated in different ways for example with recursion.
The implementation with recursion is has a complexity of O(N\^{}2) and a memory need of O(N\^{}2).
There is also an improved implementation that does not use recursion.
While the complexity is the same the memory need is reduced to O(N).
This last implementation is the implementation we selected for our experiment.
The implementation described above is also offered by a library.
This library is the Apache Commons Text library\cite{commonsText}, in our experiment we make use of this implementation for calculating the Levenshtein distance.

\subsection{Categorical data}
There are different categorical variable encoding techniques available\cite{Potdar2017AClassifiers}.
The categorical characteristics we use have no particular ranking compared to each other.
There is also no specific order to the characteristics.
I.e. a return type void is not better or worse than a return type string.
The same goes for the location characteristics, there is no location that should have a bigger weight than the other locations.
The individual characteristics do contain a finite set of values.
For example multiple mutants may contain the same class name.
The hierarchical clustering algorithm needs all characteristics in a numerical form\cite{Vijaya2019ComparativeClustering}.
The characteristics mutator identifier, class name, method name and return type are non numerical.
To deal with this problem we apply categorical variable encoding to these specific features.
Taking into account the properties of our categorical characteristics the nominal variable encoding fits our requirements.
Nominal encoding comprises a finite set of discrete values with no relationship between values\cite{Potdar2017AClassifiers}.
Therefore we implement this type of encoding in the experiment.

\subsection{Number of clusters}
Agglomerative hierarchical clustering can continue to cluster until there is one cluster left. 
Naturally this cluster will contain all the mutants.
We can cut off the clustering algorithm at any point.
We decide on the number of clusters based on the amount of mutants generated in a full set.
We select the number of clusters from a performance increasing perspective.
Mutation clustering increases the performance by reducing the amount of mutants executed\cite{Pizzoleto2019}.
By reducing the amount of mutants, for example by half, we increase the performance by 50\%\cite{Just2014AreTesting}.
As a starting point we perform three experiments where the cluster size is 25\%, 50\% and 75\% of the total amount of mutants respectively.
We can evaluate the results and can decide to cut off the clustering at different points.
The amount of mutants inside a cluster is be decided by the clustering algorithm.
\end{document}