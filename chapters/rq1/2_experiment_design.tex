\documentclass[../../main]{subfiles}
\begin{document}
% Because we use sections but these sections should be interpreted as chapters for rq1 and rq2 sectioning.
\clearpage
\section{Experiment design}
In this chapter we design an experiment to answer the first research question.
We hypothesise that we can cluster mutants with the set of characteristics we identified
while maintaining effectiveness and reducing the amount executed when executing one mutant from each cluster that is randomly selected.
Our goal is to achieve a weighted mutation score that is as close as possible to the mutation score of a full set of executed mutants.
\newline
We start with extracting the characteristics from the selected mutation testing tool. 
Next we select and implement a clustering algorithm for which we can cluster mutants.
Last we cluster the mutants and compare the mutation scores to each other. 

\subsection{Extracting characteristics}
To extract all the characteristics we identified we need to run PIT and configure it to generate all mutants that are possible within the tool. 
PIT works in phases, the first phase is the generation phase where mutants are generated.
The second phase is the execution phase where mutants are challenged by the test suite.
The last phase is the reporting phase where a report is generated based on the users preferences and data generated by PIT.
During the various stages we can extract characteristics by using PIT's plugin system.
The plugins are executed after each other and per class.
In other words first plugin A will make changes or analyse all classes then plugin B etc.
\newline
PIT offers a plugin system in which developers can inject their own code in PIT\cite{pitestPlugin}.
There are two main types of plugins; a Mutation Result Listener and a Mutation interceptor\cite{pitestPlugin}.
A mutation result listener receives the details of analysed mutations as they arrive\cite{pitestPlugin}.
A Mutation interceptor is passed a complete list of all mutation that will be generated to each class before the mutation are challenged by tests\cite{pitestPlugin}.
The implementation of our experiment makes use of this plugins system.
\newline
We developed two plugins to extract characteristics.
The first plugin is of type Mutation interceptor.
It is provided a list of details per class. 
We can extract all characteristics but the Levensthein distance and the amount of tests a mutant is challenged by.
To calculate the Levensthein distance we need the byte code of the original code and the mutated code.
We generate the byte code of the original code based on the location provided in the list of details.
We make use of existing functionality in PIT for generating byte code.
The mutator is also provided in the list of details.
With the combination of location and mutator we can generate the byte code of the mutant.
With both the mutated byte code and original byte code available we can calculate the Levenshtein distance.
\newline
The characteristics gathered until this phase of the mutation testing process are written, together with a unique identifier, to a \acrshort{csv} file.
\newline
The second plugin is a mutation result listener. 
This plugin gets passed a list of the results per mutant.
This list contains data about the mutant survival status.
It also contains data about the number of tests the mutant is challenged by.
We store this data together with the identifier to a different \acrshort{csv} file.
\newline
When the mutation testing process has finished we need to merge the characteristic gathered in from the plugins.
We add the characteristic; the number of tests the mutant is challenged by to the data file containing all the other characteristics.
All the files contain the same unique identifier per mutant on which we can merge the data.
We do not add the information about the survival of a mutant.
We only need this data to validate our results. 


\subsection{Levenshtein distance on Java byte code}
A mutant is a piece of code that differs in a predefined way from its parent.
Java source code compiles into Java byte code.
During the compilation of Java code certain context is abstracted away\cite{byteCodeEngineering}.
Optimisations are applied which also changes the Java byte code\cite{byteCodeEngineering}.
As a result the textual similarity between a Java code mutant and its parent is different from the similarity between a Java byte code mutant and its parent.
While the textual similarity may be different between the Java code and Java byte code the functionality remains the same.
Java byte code reflects more of the semantic nature of the source code than the source code itself does. 
PIT generates and executes mutants on byte code level\cite{pitestBytecode}. 
In other words PIT executes the unit tests of a source against the byte code of a mutant.
\newline
By calculating the distance on byte code we filter out the context that is present in Java code.
This gives us a Levenshtein distance that represents more of the semantic difference, between a mutant and parent, than calculating the distance for Java code.
For this characteristic we will use the Levenshtein distance between a Java byte code mutant and its parent.

\subsection{Levenshtein distance implementation}
There are different implementations of the Levensthein distance.
For our experiment we need to calculate the Levenshtein distance on the byte code generated in PIT.
Since we make use of the plugin system we also choose to use an implementation that is written in Java.
The Levenshtein distance can be calculated in different ways for example with recursion.
The implementation with recursion is has a complexity of O(N\^{}2) and a memory need of O(N\^{}2).
There is also an improved implementation that does not use recursion.
While the complexity is the same the memory need is reduced to O(N).
This last implementation is the implementation we selected for our experiment.
The implementation described above is also offered by a library.
This library is the Apache Commons Text library\cite{commonsText}, in our experiment we make use of this implementation for calculating the Levenshtein distance.


\subsection{Hierarchical clustering}
Clustering Levenshtein distances has been done before with hierarchical clustering\cite{Rajalingam2011, Gothai2010PerformanceAlgorithms}. 
Research shows that hierarchical clustering performs better when clustering with at least ten features\cite{Rodriguez2019}.
It also states that varying the parameters of hierarchical clustering improves the performance compared to that of the default settings of the algorithm\cite{Rodriguez2019}.
We have more than ten characteristics which we can use as features and adjust the parameters of the algorithm based on the characteristics in our data set.
Bases on these observations we select hierarchical clustering to cluster the mutants.
Next we will explain the configuration we use for clustering the mutants. 
\newline
Hierarchical clustering is subdivided into agglomerative and divisive. 
The agglomerative hierarchical technique follows bottom up approach whereas divisive follows top-down approaches.
Hierarchical clustering uses different metrics which calculates the euclidean distance between two clusters and the linkage criteria\cite{Rajalingam2011}. 
The linkage criteria specifies the dissimilarity in the sets as a function of the pair-wise distances of observations in those sets\cite{Rajalingam2011}.
We reviewed the different linkage criteria and concluded the following.
\newline
Research shows that the complete linkage outperforms the single linkage method\cite{Vijaya2019ComparativeClustering}.
The ward linkage and complete linkage methods perform the same when clusters are well separated\cite{Vijaya2019ComparativeClustering}.
However if the clusters overlap the ward linkage outperforms the complete linkage\cite{Vijaya2019ComparativeClustering}.
\newline
We identify all characteristics per mutant as a separate cluster.
Starting out with each mutant as a separate cluster we can use the agglomerative form of hierarchical clustering.
Since we cannot assume that our clusters are well separated we chose to use the ward linkage method for our clustering algorithm.
In summary, the agglomerative hierarchical clustering algorithm with ward linkage will cluster our mutants represented by the characteristics we gathered.


\subsection{Weighted mutation score}
\label{ch:weighted_score}
Our hypothesis states that each mutants executed should represent that whole cluster.
With a mutant executed from each cluster we can calculate a mutation score.
This mutation score is a weighted mutation score.
This weighted mutation score is the product of the result of a mutant(1 for killed and 0 for survived) and the amount of mutants in the cluster it represents.
The weighted mutation score is then comparable to the score of a full set as the total number of mutants will be the same.
\newline
For example, take a full set with a score of 75/100 killed mutants.
This gives us a mutation score of 75\%. 
We then cluster the mutants in four clusters consisting of 12, 30, 38 and 20 mutants, respectively.
We randomly select four mutants of each cluster and execute them.
The mutants representing cluster one and four survive and two and three are killed.
If we calculate the weighted score we get 68/100 which is 68\%.
We can then compare this to the score of a full set because the amount of mutants executed is the same: 75/100(75\%) and 68/100(68\%).

\subsection{Categorical data}
There are different categorical variable encoding techniques available\cite{Potdar2017AClassifiers}.
The categorical characteristics we use have no particular ranking compared to each other.
There is also no specific order to the characteristics.
I.e. a return type void is not better or worse than a return type string.
The same goes for the location characteristics, there is no location that should have a bigger weight than the other locations.
The individual characteristics do contain a finite set of values.
For example multiple mutants may contain the same class name.
The hierarchical clustering algorithm needs all characteristics in a numerical form\cite{Vijaya2019ComparativeClustering}.
The characteristics mutator identifier, class name, method name and return type are non numerical.
To deal with this problem we apply categorical variable encoding to these specific features.
Taking into account the properties of our categorical characteristics the nominal variable encoding fits our requirements.
Nominal encoding comprises a finite set of discrete values with no relationship between values\cite{Potdar2017AClassifiers}.
Therefore we implement this type of encoding in the experiment.

\subsection{Cluster size}
Agglomerative hierarchical clustering will continue to cluster until there is one cluster left. 
Naturally this cluster will contain all the mutants.
We can cut off the clustering algorithm at any point.
We decide on the number of clusters based on the amount of mutants generated in a full set.
The reasoning for the selecting the number of clusters will be from a performance reducing perspective.
Mutation clustering increases the performance by reducing the amount of mutants executed\cite{Pizzoleto2019}.
Reducing the performance by a quarter, half or three quarters will give an indication of performance increases.
To translate this to cluster size we will reduce the number of clusters in steps op 25\%.
In other words by reducing the amount of mutants, for example by half, we increase the performance by 50\%\cite{Just2014}.
If necessary we can decide to cut off the clustering at different points.
The baseline will consist of three experiments where the cluster size is 25\%, 50\% and 75\% of the total amount of mutants respectively.
The amount of mutants inside a cluster will be decided by the clustering algorithm.

\subsection{Validation}
\label{ch:exp1_validation}
The most efficient way to measure test effectiveness with mutation testing is by running all mutants that a mutation testing tool can possibly generate.
The goal of this experiment is to reduce the amount of mutants executed while maintaining effectiveness.
To reduce the amounts executed we cluster the mutants. 
We can measure this by counting the number of clusters we generate and compare it to the number of total mutants generated by the selected tool.
\newline
The baseline for our experiment is the mutation score of the set of all mutants generated by the selected mutation testing tool.
\newline
To validate how effective our clustering is we can compare the weighted mutation score(see Chapter \ref{ch:weighted_score}) of our clustered set to the mutation score of the full set.
The closer the weighted score is to that of a full set the more effective our set of characteristics and clustering algorithm proves to be.
In other words we want achieve a mutation score that is as close as possible as to that of a full set.
We select a statistical significance level of $\leq 0.05$.
This is the conventional threshold for declaring statistical significance\cite{Kirk1996PracticalCome}.
\newline
Depending on the effectiveness of our clustering algorithm we may loose accuracy.
This can happen if a cluster contains mutants of both results. 
We can measure the accuracy inside a cluster by calculating a percentage of all mutants that have survived against the ones that have been killed in a cluster.
If the majority of the mutants in a cluster is killed then we consider that cluster to represent a killed cluster and the other way around for survived mutants.
We consider the mutants that are not in the majority of the cluster as inaccuracy.
\newline
If the weighted mutation score of our clustered set deviates more than 5\% from the score of a full set we will reject our hypothesis.

\subsection{Mutant selection}
As stated in our hypothesis we will randomly select a mutant from each cluster to be executed.
To make our results reproducible we will select the random mutant based on a generated seed.
The seed generated for each run will be included in the results and can be found in the Github repository\cite{rbasarat-repo}.
We will repeat our experiment 30 times with 30 different seeds.
Achieving consistent results while applying random selection will contribute to the validity of the experiment.

\end{document}