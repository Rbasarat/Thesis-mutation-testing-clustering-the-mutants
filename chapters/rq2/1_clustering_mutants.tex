\documentclass[../../main]{subfiles}
\begin{document}

\section{Selecting a machine learning model}
In this section we select a machine learning model.
We start by choosing between supervised or unsupervised learning.
Then we look at the machine learning models that are discussed in the survey done by K.L. Du.
Based on the choice of supervised or unsupervised learning we review the corresponding models and decide if they fit our use case by a process of elimination.

\subsection{Supervised or unsupervised learning}
In this subsection we decide on supervised or unsupervised learning.
The application and description of these two types of learning are described in Chapter \ref{ch:background}.
\newline
Supervised machine learning models require a labeled data set.
We can argue that we can label our data set with the results of our first experiments.
Each mutant would then be labeled with the identity of the cluster it is clustered in by the hierarchical clustering model.
If we would train a supervised model with the data labeled as described, it may result in a model that can do the same as hierarchical clustering.
Our research is not to prove if we can train a model to cluster mutants the same way as hierarchical clustering.
We want to train a model to find the relations between the characteristics without having to define these relations manually.
\newline
Unsupervised learning models do not need a labeled data set and can recognize relations or discover hidden patterns in the characteristics we identified.
This corresponds to what we want to achieve with this experiment.
Therefore we choose unsupervised learning to continue with in our experiment.

\subsection{Mountain and subtractive clustering}
As described in Chapter \ref{ch:neuralNetworkSurvey} the mountain and subtractive clustering models calculate the amount of clusters needed for a specific data set.
Subtractive clustering is designed for learning fuzzy systems models from data\cite{Chiu1994FuzzyEstimation}. 
If it is required to find out the number of clusters we need for our model we can make use of these models.

\subsection{ART models}
While the ART models are good at what they do, they do not fit our purpose of clustering mutants.
As described in Chapter \ref{ch:neuralNetworkSurvey} the ART models expect rapidly changing sequences of input patterns.
Our data does not contain binary input patterns and it is also not rapidly changing.
Therefore we do not select an ART model.

\subsection{Neural gas}
The neural gas network is a learning network.
One of the properties of a learning network is that it is not designed to fit new data points\cite{supervisedUnsupervised}.
For this reason we cannot use the neural gas network to predict in what cluster a new mutant should be placed.
Therefore we do not select the neural gas network.

\subsection{Learning vector quantization}
The learning vector quantization model is a supervised learning model.
We established that we will use an unsupervised learning model to cluster mutants.
Therefore we do not select the supervised learning vector quantization model.

\subsection{Self organizing map, C-means and fuzzy c-means}
\label{ch:topThreeModels}
The \acrfull{som}, C-means and \acrfull{fcm} models are all models that would fit our purpose. 
To choose between these models we look at the performance of each model.
Mingoti et al., compared the performance of these three models \cite{Mingoti2006ComparingAlgorithms}.
They did a Monte Carlo simulation where the cluster sizes and amount of random numbers in the data set varied each simulation\cite{Mingoti2006ComparingAlgorithms}.
Other variables such as cluster boundaries and overlap were also controlled variables in the experiment.
They observe that the C-means and \acrlong{fcm} had good performance for non overlapping situations\cite{Mingoti2006ComparingAlgorithms}.
The best results for average recovery and internal dispersion rates were found for \acrlong{fcm} which was stable in all situations achieving recovery averages over 90\%\cite{Mingoti2006ComparingAlgorithms}
The C-means method was affected by the presence of a large amount of outliers.
They conclude that the \acrshort{som} did not perform well in many cases being affected by the amount of variables and clusters even for the non overlapping cases\cite{Mingoti2006ComparingAlgorithms}.
They also observe  that their results partially overlap with other research in the same domain.
\newline
Based the conclusions made by Mingoti et al., we can conclude that the \acrshort{fcm} model has the best performance overall.
Thus for this experiment we select the \acrshort{fcm} model.

\section{Training strategy}
\label{ch:training_strategy}
Our data set consists of mutants represented by their characteristics. 
The characteristics of the mutants are generated per project and have a project specific context.
For example, a mutant with a mutator in a certain project can have a different effect on the source code and can give other results than a mutator in a different project.
The relations between the mutants are scoped to their own project. 
In other words if a specific mutant has a relation to another mutant in the same project, this relation may not exist between the same mutant and a mutant from a different project.
To preserve these relations and context we decided to train a separate model per project.
\newline
To validate the model we need to split our data set into a training set and a validation set.
Research has shown that if the data set is big enough a ratio of 80/20 is sufficient\cite{Guyon1997ARatio}.
We divide each source projects into training and validation groups to reflect the 80/20 ratio as best as possible.
Depending on the amount of mutants per project we might not achieve a perfect 80/20 ratio.
The division of the mutants is be done randomly. 
To make our results reproducible we select a random division based on a generated seed.
The seeds generated for each run will be included in the results and can be found in the Github repository\cite{rbasarat-repo}.

\section{Implementation of algorithm}
\label{ch:fcm_implementation}
The research paper of the \acrshort{fcm} algorithm is referenced in the survey of Du et al.\cite{Du2010Clustering:Approach}.
The \acrshort{fcm} algorithm is developed by Bezdek et al.\cite{Bezdek1984FCM:Algorithm}.
Based on their research an implementation can be developed.
There are existing libraries that contain the implementation of the \acrshort{fcm} algorithm.
Developing our own version of the \acrshort{fcm} algorithm would be valuable if we want to make adjustments to the algorithm itself.
For this experiment we do not require to make adjustments to the \acrshort{fcm} algorithm.
Therefore we choose to use a library that implements the \acrshort{fcm} algorithm.
The library we will use for this experiment is the \acrlong{fcm} implementation build by Dias et al.\cite{dias2019fuzzy}.
The theory developed by Bezdek et al., is implemented in this library.
The library is actively maintained and has been used before in other research\cite{DeAlmeidaNeto2020, Kopf2019, Nwadiugwu2020}.

\section{Algorithm parameters}
The \acrshort{fcm} algorithm has to initialize the centroids of the clusters.
The library we selected does this based on a random number.
It also allows us to control this random number.
To make the initialization of the centroid reproducible we will use the same generated seeds as in our first experiment.
The seeds generated are included in the Github repository\cite{rbasarat-repo}.
\newline
Bezdek et al., also researched cluster validity for the \acrshort{fcm} model\cite{Bezdek1995OnModel}.
They researched what effect which parameters could have on the validity of the clusters.
They specifically analyzed the role of weighting exponent \textit{m}(fuzziness parameter).
They concluded that the best choice for \textit{m} is in the interval [1.5, 2.5], whose mean and midpoint is \textit{m}=2.
Wu proposed a new guideline for selecting parameter \textit{m}\cite{Wu2012}.
His point of view was that a large \textit{m} value will make the \acrlong{fcm} more robust to noise and outliers.
He suggests implementing \acrlong{fcm} with \textit{m}=[1.5, 4].
When the data set contains noise and outliers, the fuzzifier \textit{m}=4 is recommended in a theoretical upper bound case.
Since our data set does not contain noise(all mutants consists of every characteristic) we choose to select the midpoint concluded in the research of Bezdek et al.\cite{Bezdek1995OnModel}.
The value of \textit{m} in this experiment is 2.
\newline
Finding the correct amount of clusters has been an ongoing problem for clustering algorithms\cite{Du2010Clustering:Approach}.
Fortunately there are techniques available to estimate the number of clusters.
During model selection we discussed the mountain and subtractive models. 
These models are designed to estimate the number of clusters in a data set.
The subtractive clustering model is specifically designed to approximate the number of clusters for fuzzy system models\cite{Chiu1994FuzzyEstimation}.
It approximates the centroids and amount of the clusters based on the data points in the data set\cite{Chiu1994FuzzyEstimation}.
We select and implement the subtractive clustering model to estimate the number of clusters.
We use this number as the value for the number of clusters parameter in the \acrshort{fcm} algorithm.

\section{Implementation of subtractive clustering}
For the same reasons as in Chapter \ref{ch:fcm_implementation} we select a library that implements the subtractive clustering algorithm.
The research paper of the mountain and subtractive algorithms are referenced in the survey paper of Du et al.\cite{Du2010Clustering:Approach}.
Based on the paper of the theory of the subtractive algorithm\cite{Chiu1994FuzzyEstimation} we found an existing implementation\cite{matlabSubtractive}.
The implementation is professionally build and maintained by Matlab\cite{matlabSubtractive}.
Matlab is a tool designed for the academic world and is professionally maintained for this reason we decided to use their implementation of subtractive clustering\cite{matlab}.


\end{document}
