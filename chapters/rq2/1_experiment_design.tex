\documentclass[../../main]{subfiles}
\begin{document}
\section{Experiment design}
In this chapter we will design an experiment to answer the second research question.
We hypothesise that we can train a machine learning model with the set of characteristics we identified
while maintaining effectiveness and reducing the amount executed when executing one mutant from each cluster that is randomly selected.
Our goal is to achieve a mutation score that is as close as possible to the mutation score of a full set of executed mutants.
We will select a machine learning model, decide on a training strategy, train the model and validate the results.

\subsection{Selecting a machine learning model}
We will first choose between supervised or unsupervised learning.
Our starting point for selecting a machine learning model are the models discussed in the survey done by K.L. Du.
Based on the choice of supervised or unsupervised learning we will review the corresponding models and decide if they fit our use case by a process of elimination.

\subsubsection{Supervised or unsupervised learning}
In this subsection we will decide on supervised or unsupervised learning.
The application and description of these two types of learning are described in Chapter \ref{ch:background}.
\newline
Supervised machine learning models require a labeled data set.
We can argue that we can label our data set with the results of our first experiments.
Each mutant would then be labeled with the identity of the cluster it is clustered in by the hierarchical clustering model.
If we would train a supervised model with the data labeled as described, it may result in a model that can do the same as hierarchical clustering.
Our research is not to prove if we can train a model to cluster mutants the same way as hierarchical clustering.
We want to train a model to find the relations between the characteristics without having to define these relations manually.
\newline
Unsupervised learning models do not need a labeled data set and can recognize relations or discover hidden patterns in the characteristics we identified.
This corresponds to what we want to achieve with this experiment.
Therefore we choose unsupervised learning to continue with in our experiment.

\subsubsection{Mountain and subtractive clustering}
The mountain and subtractive clustering models are an effective method for estimating the number of clusters\cite{Du2010Clustering:Approach}. 
The subtractive clustering model is an extension of the mountain model\cite{Du2010Clustering:Approach}. 
\newline
The method grids the data space and computes a potential value for each grid point based on its distance to the actual data points. Each grid point is a potential cluster center. The potential for each grid is calculated based on the density of the surrounding data points. The grid with the highest potential is selected as the first cluster center and then the potential values of all the other grids are reduced according to their distances to the first cluster center.
\newline
Subtractive clustering is designed for learning fuzzy systems models from data\cite{Chiu1994FuzzyEstimation}. 
If it is required to find out the number of clusters we need for our model we can make use of these models.

\subsubsection{ART models}
ART models are characterized by systems of differential equations that formulate stable self-organizing learning methods\cite{Du2010Clustering:Approach}.
The stability and plasticity properties as well as the ability to efficiently process dynamic data make the ART attractive for clustering large, rapidly changing sequences of input patterns\cite{Du2010Clustering:Approach}.
While the ART models are very good at what they do, they do not fit our purpose of clustering mutants.
As described above the ART models expect rapidly changing sequences of input patterns.
Our data does not contain binary input patterns and it is also not rapidly changing.
Therefore we will not select an ART model.

\subsubsection{Neural gas}
The neural gas is a topology-preserving network, and can be treated as an extension to the C-means\cite{Du2010Clustering:Approach}.
A data optimal topological ordering is achieved by using neighborhood ranking within the input space at each training step\cite{Du2010Clustering:Approach}.
To find its neighborhood rank, each neuron compares its distance to the input vector with those of all the other neurons to the input vector\cite{Du2010Clustering:Approach}. 
Unlike the \acrlong{som}(see Chapter \ref{ch:topThreeModels}, which uses predefined static neighborhood relations, the neural gas network determines a dynamical neighborhood relation as learning proceeds\cite{Du2010Clustering:Approach}.
A benefit of the neural gas network is that it is able to spawn neurons.
This means that it is not necessary to know the number of clusters in advance.
Unfortunately the neural gas network is a learning network.
One of the properties of a learning network is that it is not designed to fit new data points\cite{supervisedUnsupervised}.
For this reason we cannot use the neural gas network to predict in what cluster a new mutant should be placed.
Therefore we will not select the neural gas network.

\subsubsection{Learning vector quantization}
The learning vector quantization model is a supervised learning model.
We established that we need an unsupervised learning model to cluster the mutants.
Therefore we will not select the learning vector quantization model.

\subsubsection{Self organizing map, C-means and \acrlong{fcm}}
\label{ch:topThreeModels}
The \acrfull{som}, C-means and \acrfull{fcm} models are all models that would fit our purpose. 
To choose between these models we will look at the performance of each model.
Mingoti et al., compared the performance of these three models \cite{Mingoti2006ComparingAlgorithms}.
They did a Monte Carlo simulation where the cluster sizes and amount of random numbers in the data set varied each simulation\cite{Mingoti2006ComparingAlgorithms}.
Other variables such as cluster boundaries and overlap were also controlled in the experiment.
They observe that the C-means and \acrlong{fcm} had good performance for non overlapping situations\cite{Mingoti2006ComparingAlgorithms}.
The best results for average recovery and internal dispersion rates were found for \acrlong{fcm} which was very stable in all situations achieving recovery averages over 90\%\cite{Mingoti2006ComparingAlgorithms}
The C-means method was very affected by the presence of a large amount of outliers.
They conclude that the \acrshort{som} did not perform well in many cases being very affected by the amount of variables and clusters even for the non overlapping cases\cite{Mingoti2006ComparingAlgorithms}.
They also conclude that their results partially overlap with other research in the same domain.
\newline
Based on these results we can conclude that the \acrshort{fcm} model has the best performance overall.
For this experiment we select the \acrshort{fcm} model.

\subsection{Training strategy}
\label{ch:training_strategy}
Our data set contains of mutants represented by their characteristics. 
The characteristics of the mutants are generated per project and have a project specific context.
The relations between the different mutants are also specific per project.
For example mutant with a mutator in a certain project can have a different effect on the source code and can give other results than a mutator in a different project.
To keep the relations and context we decided to train a separate model per project.
\newline
To validate the model we need to split our data set into a training set and a validation set.
Our total data set consists of approximately 1 million mutants generated over fourteen projects.
Research has shown that if the data set is big enough a ratio of 80/20 is sufficient\cite{Guyon1997ARatio}.
We will divide each source projects into training and validation groups to reflect the 80/20 ratio as best as possible.
Depending on the amount of mutants per project we might not achieve a perfect 80/20 ratio.
The division of the mutants will be done randomly. 
To make our results reproducible we will select a random division based on a generated seed.
The seed generated for each run will be included in the results and can be found in the Github repository\cite{rbasarat-repo}.

\subsection{Implementation of algorithm}
The research paper of the \acrshort{fcm} algorithm is referenced in the survey paper of Du et al\cite{Du2010Clustering:Approach}.
The \acrshort{fcm} algorithm is developed by Bezdek et al.\cite{Bezdek1984FCM:Algorithm}.
Based on their research an implementation can be developed.
There are existing libraries that contain the implementation of the \acrshort{fcm} algorithm.
Developing our own version of the \acrshort{fcm} algorithm is out of scope for this research.
For this reason we choose to use a library that implements the \acrshort{fcm} algorithm.
The library we will use for this experiment is the \acrlong{fcm} implementation build by Dias et al.\cite{dias2019fuzzy}.
The theory developed by Bezdek et al., is implemented in this library.
The library has been used in other research\cite{DeAlmeidaNeto2020, Kopf2019, Nwadiugwu2020} and is actively maintained.

\subsection{Algorithm parameters}
The \acrshort{fcm} algorithm has to initialize the centroids of the clusters.
Our selected library does this based on a random number.
It also allows us to control this random number.
To make the initialization of the centroid reproducible and deterministic we will use a generated seed.
The seed generated are included in the Github repository\cite{rbasarat-repo}.
\newline
Bezdek et al., also researched cluster validity for the \acrshort{fcm} model\cite{Bezdek1995OnModel}.
They researched what effect which parameters have on the validity of the clusters.
They specifically analyzed the role of weighting exponent \textit{m}(fuzziness parameter).
They concluded that the best choice for \textit{m} is probably in the interval [1.5, 2.5], whose mean and midpoint is \textit{m}=2.
Wu proposed a new guideline for selecting parameter \textit{m}\cite{Wu2012}.
His point of view was that a large \textit{m} value will make the \acrlong{fcm} more robust to noise and outliers.
He suggests implementing \acrlong{fcm} with \textit{m}=[1.5, 4].
When the data set contains noise and outliers, the fuzzifier\textit{m}=4 is recommended in a theoretical upper bound case.
Since our data set does not contain noise and outliers we choose to select the midpoint concluded in the research of Bezdek et al.\cite{Bezdek1995OnModel}.
The value of \textit{m} in this experiment is 2.
\newline
Finding the correct amount of clusters has been an ongoing problem for clustering algorithms\cite{Du2010Clustering:Approach}.
Fortunately there are some techniques available to estimate the number of clusters.
During model selection we discussed the mountain and subtractive models. 
These models are designed to estimate the number of clusters in a data set.
The subtractive clustering model is specifically designed to approximate the number of clusters for fuzzy system models\cite{Chiu1994FuzzyEstimation}.
It approximates the centroids and amount of the clusters based on the data points in the data set\cite{Chiu1994FuzzyEstimation}.
We will select and implement the subtractive clustering model to estimate the number of clusters.
We can use this number for the number of clusters parameter in the \acrshort{fcm} algorithm.

\subsection{Implementation of subtractive clustering}
The research paper of the mountain and subtractive algorithms are referenced in the survey paper of Du et al\cite{Du2010Clustering:Approach}.
Based on the paper of the theory of the subtractive algorithm\cite{Chiu1994FuzzyEstimation} we found an existing implementation\cite{matlabSubtractive}.
The implementation is professionally build and maintained by Matlab\cite{matlabSubtractive}.
Matlab is a tool commonly used in the academic world and is professionally maintained for this reason we decided to use their implementation of subtractive clustering.

\subsection{Validation}
To validate our machine learning model we will calculate the weighted mutation score and cluster accuracy based on the clusters calculated by the model.
This is the same validation method as described in Chapter \ref{ch:exp1_validation}.
We will use the mutants of projects that are not selected for training as input for validating the model.
As described in Chapter \ref{ch:training_strategy} the projects should represent approximately 20\% of the total amount of mutants in our data set.
It may be possible the model has to cluster mutants it has never seen before.
We will use the same statistical significance threshold as in our firsts experiment to decide whether we will accept or reject or hypothesis.
To make our results reproducible we will select a random mutant based on a generated seed.
We will repeat our experiment 30 times with 30 different seeds.
Achieving consistent results while applying random selection and cluster initialization will contribute to the validity of the experiment.
The seed generated for each run will be included in the results and can be found in the Github repository\cite{rbasarat-repo}.

\end{document}