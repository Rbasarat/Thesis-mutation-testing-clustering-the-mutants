\newif\ifdraft

% Set the below value to \drafttrue to use draft formatting or
% \draftfalse to enable "production" formatting
\draftfalse

\ifdraft
\documentclass[conference,draftclsnofoot,onecolumn]{IEEEtran}
\usepackage{fontspec}
\setmainfont{OpenDyslexic}
\else
\documentclass[conference]{IEEEtran}
\fi

\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
% \usepackage{cite}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{makecell}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage{amsmath,amssymb}
\usepackage[flushleft]{threeparttable}
\usepackage{tabularx}
\usepackage[disable]{todonotes}
\usepackage{enumitem}
\usepackage[backend=biber, style=ieee, citestyle=numeric-comp,uniquelist=false, maxcitenames=2, maxbibnames=9, mincitenames=1]{biblatex}
\usepackage{multirow}

\bibliography{../main}
\bibliography{../custom}

\usepackage{listings}
\lstset{
	basicstyle=\footnotesize,        % the size of the fonts that are used for the code
	breakatwhitespace=false,         % sets if automatic breaks should only happen at whitespace
	breaklines=true,                 % sets automatic line breaking
	captionpos=b,                    % sets the caption-position to bottom
	frame=single,                    % adds a frame around the code
	language=Java,                 % the language of the code
	keywordstyle=\bf,
	tabsize=2                       % sets default tabsize to 2 spaces
}


\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\def\code#1{\texttt{#1}}
\begin{document}

\title{Towards clustering mutants to meet CI/CD timelines}


\author{
\IEEEauthorblockN{Rasjaad Basarat}
\IEEEauthorblockA{\textit{Software Engineering Master} \\ %% TODO: CHANGE THIS
\textit{University of Amsterdam}\\
Amsterdam, the Netherlands \\
rasjaad@nivero.io}

\and
\IEEEauthorblockN{Ana Oprescu}
\IEEEauthorblockA{\textit{Complex Cyber Infrastructure} \\
\textit{University of Amsterdam}\\
Amsterdam, the Netherlands \\
a.m.oprescu@uva.nl}
\and
\IEEEauthorblockN{Thomas Biesaart}
\IEEEauthorblockA{\textit{} \\ %% TODO: CHANGE THIS
Amsterdam, the Netherlands \\
thomas.biesaart@protonmail.com}
}

\maketitle
% \todo[inline]{FROM REVIEWER: Make sure to be consistent with singular/plural (e.g. mutation operators will result in mutants, not just a mutant)}
% \todo[inline]{FROM REVIEWER:  I don't quite follow the discussion of the execution time problem. After all, even in a decentralized approach I can run multiple approaches on the same hardware and thus make a fair comparison. I suppose the point you're trying to make is comparison *across multiple papers/studies"? Please clarify.}
\begin{abstract}
Mutation testing is a computationally expensive technique that requires the ability to run test suites.
Mutation testing runs can take hours or days depending on the size of the project.
Various techniques have been proposed to reduce the cost of mutation testing.
One of these techniques is the clustering of mutants.
By clustering mutants we can execute fewer mutants to reduce the cost.
Our goal is to reduce the cost as much as possible without significantly reducing the effectiveness.

Our research identifies characteristics to represent the mutants such that we can cluster them. 
The characteristics selected indicate a relationship between mutants. 

We use two clustering approaches: a qualitative, white box, approach and a quantitative, black box, approach. The white box approach uses hierarchical clustering and for the black box approach we train a fuzzy c-means model.

We look at existing comparisons between mutation testing tools Mujava, Major and PIT.
The tools are compared on cost, effectiveness and execution time\cite{Kintis2016AnalysingStudy}.
Our goal is to generate as much mutants such that we can include as much different mutants as possible.
For this reason we extend the existing research with a measurement of mutants generated per tool. 
With the existing and extended research reviewed we selected PIT.
 
We cluster mutants and calculate a weighted mutation score and compare this with the mutation score of a full set of executed mutants.
Results show that with hierarchical clustering we can reduce the amount of mutants executed while maintaining the test suite effectiveness.
The clusters generated by the machine learning model were less accurate and showed a significant decrease in test suite effectiveness.

% Our research consists of two parts in which we cluster mutants. To cluster mutants we need a mutation testing tools.
% Three mutation testing tools for Java are compared and one is selected for further use in this research.
% Our research uses the mutation testing tool PIT.
% We identified characteristics to represent the mutants such that we can cluster them.
% These characteristics have shown that there exists a strong relationship between mutants. 
% We prove this by clustering them with two different approaches; a white box approach and a black box approach.
% For our white box approach we used hierarchical clustering and for our black box approach we trained the fuzzy c-means model.
% We calculated a weighted mutation score and compared this with the mutation score of a full set of mutants executed.
% Results show that with hierarchical clustering we can reduce the amount of mutants executed while maintaining the effectiveness.
% The clusters generated by the machine learning model were less accurate and showed a significant decrease in effectiveness.
\end{abstract}

\begin{IEEEkeywords}

\end{IEEEkeywords}

\section{Introduction}
The high temporal cost requirement is often a barrier for adopting mutation testing~\cite{Pizzoleto2019}.
Many techniques and methods have been developed to improve the performance, such as excluding certain mutants from a mutation testing run. Most of these approaches are not as effective as testing a full set of mutants, which, on the other hand, can take hours or days depending on the size of the project~\cite{Pizzoleto2019,Yao2014}.

Another technique to reduce the number of mutants, and thus the number of test case executions is mutation clustering~\cite{Ma2016,Yu2019PossibilityScope}, which has been researched with promising results~\cite{Ji2009,Wilinski2015,Ma2016}. 
However, the reduction in temporal cost is still insufficient \todo{give an indication of how it would take compared to how much it should take to fit the CI/CD typical timeline} when considering CI/CD pipelines. 
When developing in a team waiting hours for a mutation test to complete is not practical, it will slow down development significantly. 
Thus the cost outweighs the benefits.

To reduce the cost of mutation testing, we try to find a solution that can cluster mutants while maintaining the accuracy of the same complete set of mutants. 
Our goal is to cluster every mutant that is generated within the Java programming language.
This research aims to remove the scoping limitations present in existing research.
There should be no mutant excluded from clustering to ensure the validity of the mutation score.
We achieve this using two different techniques: a qualitative, white box approach, and a quantitative, black box approach.
The white box approach contains a qualitative analysis on mutants and a methodology to cluster them.
The black box approach makes use of a machine learning model to cluster mutants. We also analyse several mutation testing tools before selecting one for conducting the research in this paper.

We structure our research on the following questions:

\textbf{Research Question 1}: What set of characteristics can we identify for clustering generated mutants to reduce the amount executed, while maintaining effectiveness?

\textbf{Research Question 2}: How can we train a machine learning model to recognize and cluster generated mutants to reduce the amount executed, while maintaining effectiveness?

\subsection{Contributions}
Our research makes the following contributions:
\begin{enumerate}
 \item A white box methodology to cluster generated mutants based on chosen characteristics to reduce the cost of mutation testing.
  \item A black box methodology to cluster generated mutants based on chosen characteristics to reduce the cost of mutation testing.
 \item A proof of concept which implements the qualitative methodology chosen and elaborated within the thesis.
 \item A proof of concept which implements the quantitative approach to cluster mutants and is elaborated within the thesis. 
\end{enumerate}
\subsection{Outline}

\section{Selecting a mutation testing tool}
For this research, we need a tool that generates and executes mutants.
Without a tool to execute and generate mutants we cannot calculate mutation scores. 
Comparing the mutation scores gives us a measurement on the effectiveness of the clusters that we will generate.
Our goal is to find the tool that generates the most amount of mutants.
By not excluding any mutants we can ensure the effectiveness of mutation testing.

We reviewed literature, by searching for state of the art research on the comparison of mutation testing tools for Java\cite{thesis}.
There are three candidate tools \cite{Kintis2016AnalysingStudy,Marki2017MutationJava}: Major\cite{Major}, MuJava\cite{mujava} and PIT\cite{pit}.
The most recent study is from 2017. 
Some of these tools were updated or are still under active development\cite{pit-releases,Major}.
We extend the existing comparison with the operators that have been added after the publishing date of the existing literature.
Table~\ref{tab:mutators-current} shows the comparison of operators that were added.

\begin{table*}[ht]
\centering
\begin{tabular}{|l|l|l|l|}
\hline
\multicolumn{2}{|c|}{\textbf{MuJava}}                           & \multicolumn{2}{|c|}{\textbf{PIT}}                      \\ \hline
IHD & Hiding variable deletion                                  & EM                    & Empty Returns                   \\ \hline
IHI & Hiding variable   insertion                               & FR                    & False Returns                   \\ \hline
IOD & Overriding method   deletion                              & NR                    & Null Returns                    \\ \hline
IOP & Overriding method   calling position change               & TR                    & True Returns                    \\ \hline
IOR & Overriding method   rename                                & PR                    & Primitive Returns               \\ \hline
ISI & Super keyword insertion                                   & ER                    & Experimental Switch             \\ \hline
ISD & Super keyword deletion                                    & BI                    & Big Integer                     \\ \hline
IPC & Explicit call to a   parentâ€™s constructor deletion        & NRC                   & Naked Receiver                  \\ \hline
PNC & New method call with   child class type                   & N                     & Negation                        \\ \hline
PMD & Member variable   declaration with parent class type      & AOR                   & Arithmic Operator Replacement   \\ \hline
PPD & Parameter variable   declaration with child class type    & AOD                   & Arithmic Operator Deletion      \\ \hline
PCI & Type cast operator   insertion                            & CR                    & Constant replacement            \\ \hline
PCC & Cast type change                                          & BO                    & Bitwise Operator                \\ \hline
PCD & Type cast operator   deletion                             & ROR                   & Relational Operator Replacement \\ \hline
PRV & Reference assignment   with other comparable variable     & UOI                   & Unary Operation Insertion       \\ \hline
OMR & Overloading method   contents replace                     & \multicolumn{1}{l|}{} &                                 \\ \hline
OMD & Overloading method   deletion                             & \multicolumn{1}{l|}{} &                                 \\ \hline
OAC & Arguments of   overloading method call change             & \multicolumn{1}{l|}{} &                                 \\ \hline
JTI & This keyword insertion                                    & \multicolumn{1}{l|}{} &                                 \\ \hline
JTD & This keyword deletion                                     & \multicolumn{1}{l|}{} &                                 \\ \hline
JSI & Static modifier   insertion                               & \multicolumn{1}{l|}{} &                                 \\ \hline
JSD & Static modifier   deletion                                & \multicolumn{1}{l|}{} &                                 \\ \hline
JID & Member variable   initialization deletion                 & \multicolumn{1}{l|}{} &                                 \\ \hline
JDC & Java-supported default   constructor deletion             & \multicolumn{1}{l|}{} &                                 \\ \hline
EOA & Reference assignment   and content assignment replacement & \multicolumn{1}{l|}{} &                                 \\ \hline
EOC & Reference comparison   and content comparison replacement & \multicolumn{1}{l|}{} &                                 \\ \hline
EAM & Acessor method change                                     & \multicolumn{1}{l|}{} &                                 \\ \hline
EMM & Modifier method change                                    & \multicolumn{1}{l|}{} &                                 \\ \hline
\end{tabular}
\caption{\label{tab:mutators-current}New mutators not taken into account in previous research.}
\end{table*}
There is some overlap between the new mutators of MuJava and PIT. These mutators have different names but mutate the code in the same way. Table \ref{tab:mutators-overlap} shows an overview of overlapping mutators.

\begin{table*}[ht]
\centering
\begin{tabular}{|l|l|l|l|}
\hline
\multicolumn{2}{|c|}{\textbf{ MuJava mutator}}      & \multicolumn{2}{c|}{\textbf{PIT mutator}}                         \\ \hline
AORB & Arithmetic Operator Replacement Binary      & \multirow{4}{*}{M}   & \multirow{4}{*}{Math}                      \\ \cline{1-2}
ASRS & Short-Cut Assignment   Operator Replacement &                      &                                            \\ \cline{1-2}
SOR  & Shift Operator   Replacement                &                      &                                            \\ \cline{1-2}
COR  & Conditional Operator   Replacement          &                      &                                            \\ \hline
AOIU & Arithmetic Operator   Insertion Unary       & \multirow{2}{*}{UOI} & \multirow{2}{*}{Unary Operation Insertion} \\ \cline{1-2}
AOIS & Arithmetic Operator   Insertion Short-cut   &                      &                                            \\ \hline
ODL  & Operator Deletion                           & AOD                  & Arithmic Operator   Deletion               \\ \hline
AODS & Arithmetic Operator   Deletion Short-cut    & RI                   & Remove Increments                          \\ \hline
ROR  & Relational Operator   Replacement           & CB                   & Conditionals Boundary                      \\ \hline
AORS & Arithmetic Operator   Replacement Short-Cut & I                    & Increments                                 \\ \hline
COD  & Conditional Operator   Deletion             & RC                   & Remove Conditionials                       \\ \hline

\end{tabular}
\caption{\label{tab:mutators-overlap}Overview of mutator overlaps between  MuJava and PIT}
\end{table*}

Our goal is to cluster as many mutants as possible, we note how many mutants each tool generates on the same source (numbers shown in Table~\ref{tab:mutants-generated}).
\begin{table}[ht]
\centering
\begin{tabular}{|l|l|c|c|c|}
\hline
                                & \textbf{version} & \textbf{\begin{tabular}[c]{@{}c@{}}LOC\end{tabular}} & \multicolumn{1}{l|}{\textbf{MuJava}} & \multicolumn{1}{l|}{\textbf{PIT}} \\ \hline
Commons-numbers.core            & 1.0-beta1        & 450                                                                    & 988                                  & 4384                              \\ \hline
Jodatime                        & 2.10.10          & 28811                                                                  & 52925                                & 112772                            \\ \hline
Zxing 3.4.1                     & 3.4.1            & 24792                                                                  & 65983                                & 161409                            \\ \hline
Google Auto Common              & 0.11.0           & 2338                                                                   & 20                                   & 5219                              \\ \hline
Google Auto Factory             & 1.0-beta9        & 1507                                                                   & 69                                   & 5832                              \\ \hline
Google Auto Value               & 1.7.5            & 7466                                                                   & 745                                  & 16746                             \\ \hline
Google Auto Service             & 1.0-rc6          & 750                                                                    & 0                                    & 475                               \\ \hline
ScribeJava-Core                 & 8.1.0            & 5709                                                                   & 1358                                 & 5746                              \\ \hline
Checkstyle                      & 8.41.1           & 38491                                                                  & 4529                                 & 100952                            \\ \hline
Fastjson                        & 1.2.75           & 43405                                                                  & 70577                                & 116188                            \\ \hline
Jfreechart                      & 1.5.3            & 91876                                                                  & 0                                    & 350741                            \\ \hline
Commons-lang                    & 3.12.0           & 29836                                                                  & 31145                                & 134764                            \\ \hline
Commons-codec                   & 1.15.0           & 9656                                                                   & 21719                                & 54804                            \\ \hline
Commons-text                   & 1.9.0             & 9781                                                                   & 10403                                & 48490                            \\ \hline
Commons-io                     & 2.8.0             & 13947                                                                  & 13245                                & 44631                            \\ \hline
Gson                           & 2.8.6             & 8078                                                                   & 9198                                 & 28485                            \\ \hline
Commons-cli                    & 1.4.0             & 2782                                                                   & 831                                  & 7193                             \\ \hline
Commons-csv                    & 1.8.0             & 1855                                                                   & 2081                                 & 6906                             \\ \hline
\end{tabular}
\caption{\label{tab:mutants-generated}Amount of mutants generated by MuJava and PIT, LOC = Lines of code}
\end{table}

MuJava is not actively maintained and has not been updated in the last few years.
It does not support source projects with java version 1.7 or higher, nor JUnit 4 or any version of TestNG~\cite{mujava}.
These are test engines and are crucial for executing tests..
All these constraints would reduce the set of projects we could use in our experiments.

While Major supports JUnit 4,
we did not succeed in generating mutants with this tool~\cite{thesis}.
It would require customizing each source project, and
there is not much documentation available for this tool.

PIT is industry-minded, actively maintained and open source~\cite{Kintis2016AnalysingStudy}.
It supports Maven, Gradle, has a command line interface and has a faster execution time than the other tools.
For example PIT provides a plugin system in which you can inject your own code in various stages of mutation testing process~\cite{pit}
We can use this feature to adjust the mutation process and include our experiments.
PIT also generated significantly more mutants(see Table \ref{tab:mutants-generated}) in every project we have tested~\cite{thesis}.
We therefore decide to use PIT for generating and executing mutants in this research.

\section{Qualitative clustering approach}
\label{ch:reserach_question1}
Our goal is to cluster every generated mutant while maintaining effectiveness. We perform a qualitative analysis on mutants to identify characteristics that represent the mutants and we devise a methodology for clustering mutants based on these characteristics. 

\subsection{Identifying characteristics}
\label{ch:identifying_characteristics}
Zhang et al. identify several mutant characteristics~\cite{Zhang2019PredictiveTesting}. 
The goal of their research is detecting as accurately as possible whether a mutant survives before executing the mutant.
The mutant characteristics identified by Zhang et al., were used in other research with promising results~\cite{Oonk2021}.
Our goal is to group mutants based on characteristics, this is different than that of Zha, however we can still use these characteristics for clustering.
We select characteristics relevant to our research from the research of Zhang et al., and Oonk~\cite{Oonk2021}.
We extend the list of characteristics with data we can extract from the selected mutation testing tool. 

PIT generates and executes mutants on byte code level~\cite{pitestBytecode}.
By mutating in Java byte code we can identify characteristics specific to Java byte code. PIT also gathers mutant metadata. We can extract this mutant metadata and identify more characteristics.  With the combination of information extracted from PIT and from existing research we identify the following characteristics~\cite{thesis}:
\begin{enumerate}
    \item Mutant similarity.
    \item Amount of tests the mutant is challenged by~\cite{Zhang2019PredictiveTesting, Oonk2021, pit}.
    \item Mutator identifier~\cite{pit}.
    \item Mutant opcode~\cite{Oonk2021, pit}.
    \item Mutant return type~\cite{Zhang2019PredictiveTesting, Oonk2021, pit}.
    \item Mutant contains local variables~\cite{pit}.
    \item Mutant is in try catch block~\cite{pit}.
    \item Mutant is in finally block~\cite{pit}.
    \item Name of class that contains the mutant~\cite{Oonk2021, pit}.
    \item Name of function that contains the mutant~\cite{Oonk2021, pit}.
    \item Line number of the start of the block that contains the mutant~\cite{pit}.
    \item Line number on which the mutation occur~\cite{pit}.
\end{enumerate}

\subsection{Hierarchical clustering}
Research shows that hierarchical clustering performs better when clustering with at least ten features and that varying the parameters of hierarchical clustering improves the performance compared to that of the default settings of the algorithm~\cite{Rodriguez2019}.

We identified more than ten characteristics and can use them as features. We adjust the parameters of the algorithm based on the characteristics in our data set.
Based on these observations and prior research, we select hierarchical clustering\todo{do you have experiments showing best performance in terms of partitioning?}.
% as it gives us the best performance in terms of partitioning.

Next we explain the configuration we use for clustering the mutants. 
Hierarchical clustering is subdivided into agglomerative and divisive. 
The agglomerative hierarchical technique follows a bottom up approach whereas divisive follows top-down approaches.
Hierarchical clustering uses different metrics which calculates the Euclidean distance between two clusters and the linkage criteria~\cite{Rajalingam2011}. 
The linkage criteria specifies the dissimilarity in the sets as a function of the pair-wise distances of observations in those sets.

Research shows that the complete linkage outperforms the single linkage method~\cite{Vijaya2019ComparativeClustering}. The ward linkage and complete linkage methods perform the same when clusters are well separated, however, if the clusters overlap the ward linkage outperforms the complete linkage~\cite{Vijaya2019ComparativeClustering}.

Each mutant is represented by its characteristics, the mutants start in their own cluster of size one.
Starting out with each mutant as a separate cluster we can use the agglomerative form of hierarchical clustering.
Since we cannot assume that our clusters are well separated we chose to use the ward linkage method for our clustering algorithm.
In summary, we select the agglomerative hierarchical clustering algorithm with ward linkage to cluster mutants represented by the characteristics we gathered\todo{perhaps a figure would help explain it better?}.

\subsection{Categorical data}
There are different categorical variable encoding techniques available~\cite{Potdar2017AClassifiers}.
The selected categorical characteristics have  no specific order and no particular ranking compared to each other, e.g., a return type void is not better or worse than a return type string.
% There is also no specific order to the characteristics.
The same applies to the location characteristics, there is no location that should have a bigger weight than the other locations.

The individual characteristics do contain a finite set of values. For example multiple mutants may contain the same class name. The hierarchical clustering algorithm needs all characteristics in a numerical form~\cite{Vijaya2019ComparativeClustering}.
The characteristics mutator identifier, class name, method name and return type are non numerical.
To deal with this problem we apply categorical variable encoding to these specific features.
Taking into account the properties of our categorical characteristics the nominal variable encoding fits our requirements. Nominal encoding comprises a finite set of discrete values with no relationship between values~\cite{Potdar2017AClassifiers}.
Therefore we implement this type of encoding in the experiment\todo{if time allows, revisit for clarity between categorical variable encoding and nominal variable encoding: which do we use? or both?}.

\subsection{Number of clusters}
Agglomerative hierarchical clustering can continue to cluster until there is one cluster left. 
Naturally this cluster will contain all the mutants.
We can cut off the clustering algorithm at any point.
We decide on the number of clusters based on the amount of mutants generated in a full set.
We select the number of clusters from a performance increasing perspective.
Mutation clustering increases the performance by reducing the amount of mutants executed~\cite{Pizzoleto2019}.
By reducing the amount of mutants, for example by half, we increase the performance by 50\%~\cite{Just2014AreTesting}.
The amount of mutants inside a cluster and the number of clusters is be decided by the clustering algorithm.
As a starting point we perform three experiments where the cluster size is 25\%, 50\% and 75\% of the total amount of mutants respectively.


\section{Quantitative clustering approach}
\label{ch:reserach_question2}
In the following sections we devise a black box approach for clustering mutants. While our goal remains the same as in Section~\ref{ch:reserach_question1}, the method to achieve it is different. We select a machine learning model by first choosing between supervised or unsupervised learning.
Then we look at the machine learning models that are discussed in the survey conducted by K.L. Du~\cite{Du2010Clustering:Approach}\todo{this is a 10 year old survey, why not something more recent?}.
We reuse of the characteristics identified in 
Section~\ref{ch:identifying_characteristics}.

\subsection{Supervised or unsupervised learning}
Supervised machine learning models require a labeled data set. We could label our data set with the results of our first approach. Each mutant would then be labeled with the identity of the cluster assigned by the hierarchical clustering model. If we would train a supervised model with the data labeled as described, it may result in a model that can do the same as hierarchical clustering.
However, we did not set out to prove whether we can train a model to cluster mutants the same way as hierarchical clustering. The goal is to train a model to find the relations between the characteristics without having to define these relations manually.

Unsupervised learning models do not need a labeled data set and can recognize relations or discover hidden patterns in the characteristics we identified.
This fits better the goal of our research and
therefore we choose unsupervised learning.

\subsection{Self organizing map, C-means and fuzzy c-means}
\label{ch:topThreeModels}
We reviewed the models discussed by K.L. Du~\cite{Du2010Clustering:Approach, thesis} and select three models which best fit our use case:
the self-organizing map, C-means and fuzzy c-means. 
To choose between these models we look at their performance. Mingoti et al. compared the performance of these three models~\cite{Mingoti2006ComparingAlgorithms}.
% They performed a Monte Carlo simulation where the cluster sizes and amount of random numbers in the data set varied each simulation. Other variables such as cluster boundaries and overlap were also controlled variables in the experiment. They observe that the C-means and fuzzy c-means had good performance for non overlapping situations. The best results for average recovery and internal dispersion rates were found for fuzzy c-means which was stable in all situations achieving recovery averages over 90\%\cite{Mingoti2006ComparingAlgorithms}
% The C-means method was affected by the presence of a large amount of outliers.
The best results for average recovery and internal dispersion rates were found for fuzzy c-means which was stable in all situations achieving recovery averages over 90\%. The C-means method was affected by the presence of a large amount of outliers, and the self-organizing map did not perform well in many cases being affected by the amount of variables and clusters even for the non overlapping cases. We therefore select the fuzzy c-means model.

\subsection{Training strategy}
\label{ch:training_strategy}
Our data set consists of mutants represented by their characteristics. 
The characteristics of the mutants are generated per project and have a project specific context.
For example, a mutant with a mutator in a certain project can have a different effect on the source code and can give other results than a mutator in a different project.
The relations between the mutants are scoped to their own project. 
In other words if a specific mutant has a relation to another mutant in the same project, this relation may not exist between the same mutant and a mutant from a different project. To preserve these relations and context we decided to train a separate model per project.

To validate each model we need to split our data set into a training set and a validation set.
Research has shown that if the data set is big enough a ratio of 80/20 is sufficient~\cite{Guyon1997ARatio}.
We divide each source projects into training and validation groups to reflect the 80/20 ratio as best as possible. Depending on the amount of mutants per project we might not achieve a perfect 80/20 ratio.
The division of the mutants is done randomly, using a controlled random seed (see Section~\ref{ch:exp1_validation}).
% that is included in the results for reproducibility (file \textit{hierarchical-clustering/main.py}, line 182~\cite{rbasarat-repo}).

\subsection{Algorithm parameters}
\label{ch:algorithm_parameters}
The fuzzy c-means algorithm has to initialize the centroids of the clusters. The implementation library uses a random number that can be controlled.
% To make the initialization of the centroid reproducible we use the same generated seeds as mentioned above.

Bezdek et al. also researched cluster validity for the fuzzy c-means model~\cite{Bezdek1995OnModel}.
They researched what effect which parameters could have on the validity of the clusters and 
% . They specifically analyzed the role of weighting exponent \textit{m}(fuzziness parameter).
concluded that the best choice for \textit{m} is in the interval [1.5, 2.5], whose mean and midpoint is \textit{m}=2.
Wu proposed a new guideline for selecting parameter \textit{m}~\cite{Wu2012}.
His point of view was that a large \textit{m} value will make the fuzzy c-means more robust to noise and outliers.
He suggested implementing fuzzy c-means with \textit{m}=[1.5, 4].
When the data set contains noise and outliers, the fuzzifier \textit{m}=4 is recommended in a theoretical upper bound case.
Since our data set does not contain noise(all mutants consists of every characteristic) we choose to select the midpoint as recommended by Bezdek et al.~\cite{Bezdek1995OnModel}. The value of \textit{m} in this experiment is 2.

Finding the correct amount of clusters has been an ongoing problem for clustering algorithms~\cite{Du2010Clustering:Approach}.
Fortunately there are techniques available to estimate the number of clusters.
The subtractive clustering model is specifically designed to approximate the number of clusters for fuzzy system models~\cite{Chiu1994FuzzyEstimation}.
It approximates the centroids and amount of the clusters based on the data points in the data set~\cite{Chiu1994FuzzyEstimation}.
We select and implement the subtractive clustering model to estimate the number of clusters.
We use this number as the value for the number of clusters parameter in the fuzzy c-means algorithm.

\section{Experimental setup and design}
We design an experiment to cluster mutants and measure the mutation score with the clustered set. 
We execute the following steps:
\begin{enumerate}
    \item Gather all mutant characteristics from PIT.
    \item Execute full set of mutants to gather baseline measurement.
    \item Cluster the mutants according the black and white box methodologies.
    \item Execute one random mutant per cluster and gather result.
    \item Calculate weighted mutation score.
\end{enumerate}
Both research questions have the same goal, but with different approaches. We devise one hypothesis for both research questions: Mutants can be clustered with the set of selected characteristics while maintaining effectiveness, thus reducing the amount executed by executing one mutant from each cluster that is randomly selected.
Our goal is to achieve a weighted mutation score that is as close as possible to the mutation score of a full set of executed mutants.
We repeat the experiment for both the white and black box approaches.

\subsection{Baseline measurement}
The baseline for our experiment is the mutation score of the set of all mutants generated by PIT.
% To extract this score we need to execute a full set of mutants within PIT.
For each source we have added the PIT plugin and required configuration in the \textit{pom.xml}, this code instructs PIT to generate and execute mutants with all mutators available.
We use the command line interface and a maven command(see Listing \ref{lst:mvn-full}) to execute PIT.
\begin{lstlisting}[label=lst:mvn-full,caption=Command to execute full set of mutants with PIT.]
mvn -U org.pitest:pitest-maven:mutationCoverage
\end{lstlisting}

\subsection{Weighted mutation score}
\label{ch:weighted_score}
Our hypothesis entails that each executed mutant should represent that whole cluster. With a mutant executed from each cluster we can calculate a weighted mutation score as the product of the result of a mutant (1 for killed and 0 for survived) and the amount of mutants in the cluster it represents. The weighted mutation score is then comparable to the score of a full set as the total number of mutants will be the same.

For example, take a full set with a score of 75/100 killed mutants. This gives us a mutation score of 75\%. 
Assume mutants are clustered in four clusters consisting of 12, 30, 38 and 20 mutants, respectively.
We randomly select one mutant of each cluster(four in total) and execute them. The mutants representing cluster one and four survive, while two and three are killed.
If we calculate the weighted score we get 68/100 which is 68\%. We can then compare the weighted score to the score of a full set because the amount of mutants in each score is the same: 75/100(75\%) and 68/100(68\%).

\subsection{Random mutant selection}
\label{ch:mutant_selection}
To validate the random selection of mutants we make use of statistical hypothesis testing~\cite{Emmert-Streib2019UnderstandingInference}. Our null hypothesis states that there is no relation between the characteristics identified and the results of a mutant with the chosen methodology.
To test this hypothesis we select an alpha value of 0.05.
% To translate: if more than five percent of the values in a sample deviates more than the significance level(see Chapter \ref{ch:exp1_validation}) we cannot reject the null hypothesis.
To make our results reproducible we select the random mutant based on a generated seed.
The seed generated for each run will be included in the results and can be found in the Github repository in the file \textit{hierarchical-clustering/main.py} at line 182\cite{rbasarat-repo}.
We repeat our experiment 30 times with 30 different seeds.
Achieving consistent results while applying random selection contributes to the validity of the experiment.

\subsection{Evaluation}
\todo[inline]{by the way, you mention nothing about collecting the runtimes?}
\label{ch:exp1_validation}
The accurate way to measure test effectiveness with mutation testing is by executing all mutants that a mutation testing tool can possibly generate.
% The goal of this research is to reduce the amount of mutants executed while maintaining effectiveness.
To reduce the amounts executed we cluster the mutants. 
We measure the reduction factor by counting the number of clusters we generate and compare it to the number of total mutants generated by the selected tool.

To validate how effective our method of clustering is we compare the weighted mutation score (see Section~\ref{ch:weighted_score}) of the clustered set to the mutation score of the full set.
The closer the weighted score is to that of a full set the more effective our set of characteristics and clustering algorithm proves to be.
% In other words our goal is to achieve a mutation score that is as close as possible as to that of a full set.
We select a statistical significance level of $\leq 0.05$.
This is the conventional threshold for declaring statistical significance~\cite{Kirk1996PracticalCome}.

Depending on the performance of our clustering algorithm, in terms of clustering accuracy, we may lose accuracy in test effectiveness. 
This can happen if a cluster contains mutants of both results; killed and sruvived. 
We can measure the accuracy inside a cluster by calculating a percentage of all mutants that have survived against the ones that have been killed in a cluster.
If the majority of the mutants in a cluster is killed then we consider that cluster to represent a killed cluster and the other way around for survived mutants.
We consider the mutants that do not have the same result as the majority of results of the cluster as inaccuracy.
If the weighted mutation score of our clustered set deviates by more than 5\% from the score of a full set we reject our hypothesis \todo{why 5?}.

\section{Results}
\label{ch:results}
% In this chapter we present the results of our experiments.
We divide the results in two sections. The first section presents the results of clustering with hierarchical clustering. The second section presents the results of clustering with the fuzzy c-means model.
We select the Levenshtein distance for our similarity characteristic\cite{thesis}.
While extracting the set of characteristics (Section~\ref{ch:identifying_characteristics}, we observed that calculating the Levenshtein distance is computationally expensive.
% Calculating the Levenshtein distance for the four biggest projects in the data set cost days\cite{thesis}.
% Even for the smaller projects the extraction of the characteristics cost hours\cite{thesis}.
Including the Levenshtein distance defeats the purpose of increasing performance as every bit of performance we gain would be replaced by the cost of calculating the Levenshtein distance~\cite{thesis}.
All results displayed are in percentages and ms=mutation score, ca=cluster accuracy.

\subsection{Hierarchical clustering}
We present the results of our clustering algorithm when executed without the Levenshtein distance.
The results are grouped based on the reduced percentage of mutants executed.
We also display the accuracy of the clusters per reduction amount.
The data we present are the average of the results of the 30 repetitions of the experiment.
The result of each single repetition can be found in our Github repository in the folder\textit{experiment{\_}results}~\cite{rbasarat-repo}.


\begin{figure}[ht]
\includegraphics[width=0.5 \textwidth]{images/boxplot_summary/boxplot_hc_no_distance_0.25.png}
\caption{\label{box:clustering_no_distance_25}Box-plots containing weighted mutation score of clustering mutants without Levensthein distance and cluster size n*0.25}
\end{figure}

\begin{table*}[ht]
\centering
\begin{tabular}{|l|c|c|c|c|c|}
\hline
\multicolumn{1}{|c|}{Project} & \begin{tabular}[c]{@{}c@{}}ms\\ full set\end{tabular} & \begin{tabular}[c]{@{}c@{}}ms \\ clustered set\end{tabular} & \begin{tabular}[c]{@{}c@{}}Avg. ca\end{tabular} & \begin{tabular}[c]{@{}c@{}}Min. ca\end{tabular} & \begin{tabular}[c]{@{}c@{}}ca\end{tabular} \\ \hline
Google Auto Service           & 0.00                                                                                  & 0.00                                                                                      & 0.00                                                                               & 0.00                                                                                & 0.00                                                                               \\ \hline
ScribeJava-Core               & 33.52                                                                                 & 33.72                                                                                     & 92.24                                                                              & 50.00                                                                               & 100.00                                                                             \\ \hline
Google Auto Factory           & 48.41                                                                                 & 48.32                                                                                     & 94.21                                                                              & 50.00                                                                               & 100.00                                                                             \\ \hline
Google Auto Common            & 59.59                                                                                 & 60.03                                                                                     & 91.19                                                                              & 50.00                                                                               & 100.00                                                                             \\ \hline
Google Auto Value             & 61.42                                                                                 & 62.58                                                                                     & 93.90                                                                              & 50.00                                                                               & 100.00                                                                             \\ \hline
Google Gson                   & 66.83                                                                                 & 66.89                                                                                     & 91.75                                                                              & 50.00                                                                               & 100.00                                                                             \\ \hline
commons-io                    & 69.70                                                                                 & 66.08                                                                                     & 90.20                                                                              & 50.00                                                                               & 100.00                                                                             \\ \hline
commons-cli                   & 79.07                                                                                 & 78.97                                                                                     & 91.11                                                                              & 50.00                                                                               & 100.00                                                                             \\ \hline
commons-text                  & 79.29                                                                                 & 79.30                                                                                     & 90.95                                                                              & 50.00                                                                               & 100.00                                                                             \\ \hline
commons-codec                 & 84.56                                                                                 & 84.59                                                                                     & 94.56                                                                              & 50.00                                                                               & 100.00                                                                             \\ \hline
commons-csv                   & 85.16                                                                                 & 85.24                                                                                     & 92.76                                                                              & 50.00                                                                               & 100.00                                                                             \\ \hline
\end{tabular}
\caption{\label{tab:clustering_no_distance_25}Results of clustering mutants without Levensthein distance and cluster size n*0.25}
\end{table*}

Figure \ref{box:clustering_no_distance_25} displays the box-plots of the weighted mutation score obtained from each individual sample.
We observe that for every box-plot the \textit{p-value} is below 0.05 or 5\%.
Table~\ref{tab:clustering_no_distance_25} displays the results of clustering mutants without Levensthein distance and the number of clusters equal to the total amount of mutants * 0.25. 
We observe that the maximum and minimum differences in mutation score between a full set and a clustered set  3.61\%, and 0.09\%, respectively.
The average of the differences between full set score and clustered score is 0.53\%.

\begin{figure}[ht]
\includegraphics[width=0.5 \textwidth]{images/boxplot_summary/boxplot_hc_no_distance_0.5.png}
\caption{\label{box:clustering_no_distance_50}Box-plots containing weighted mutation score of clustering mutants without Levensthein distance and cluster size n*0.50}
\end{figure}

\begin{table*}[ht]
\centering
\begin{tabular}{|l|c|c|c|c|c|}
\hline
\multicolumn{1}{|c|}{Project} & \begin{tabular}[c]{@{}c@{}}ms\\ full set\end{tabular} & \begin{tabular}[c]{@{}c@{}}ms \\ clustered set\end{tabular} & \begin{tabular}[c]{@{}c@{}}Avg. ca\end{tabular} & \begin{tabular}[c]{@{}c@{}}Min. ca\end{tabular} & \begin{tabular}[c]{@{}c@{}}ca\end{tabular} \\ \hline
Google Auto Service           & 0.00                                                                                  & 0.00                                                                                      & 0.00                                                                               & 0.00                                                                                & 0.00                                                                               \\ \hline
ScribeJava-Core               & 33.52                                                                                 & 33.55                                                                                     & 97.16                                                                              & 50.00                                                                               & 100.00                                                                             \\ \hline
Google Auto Factory           & 48.41                                                                                 & 48.44                                                                                     & 97.71                                                                              & 50.00                                                                               & 100.00                                                                             \\ \hline
Google Auto Common            & 59.59                                                                                 & 59.86                                                                                     & 96.35                                                                              & 50.00                                                                               & 100.00                                                                             \\ \hline
Google Auto Value             & 61.42                                                                                 & 62.62                                                                                     & 97.31                                                                              & 50.00                                                                               & 100.00                                                                             \\ \hline
Google Gson                   & 66.83                                                                                 & 66.81                                                                                     & 95.29                                                                              & 50.00                                                                               & 100.00                                                                             \\ \hline
commons-io                    & 69.70                                                                                 & 66.05                                                                                     & 95.08                                                                              & 50.00                                                                               & 100.00                                                                             \\ \hline
commons-cli                   & 79.07                                                                                 & 78.97                                                                                     & 95.55                                                                              & 50.00                                                                               & 100.00                                                                             \\ \hline
commons-text                  & 79.29                                                                                 & 79.30                                                                                     & 95.34                                                                              & 50.00                                                                               & 100.00                                                                             \\ \hline
commons-codec                 & 84.56                                                                                 & 84.59                                                                                     & 96.95                                                                              & 50.00                                                                               & 100.00                                                                             \\ \hline
commons-csv                   & 85.16                                                                                 & 85.11                                                                                     & 95.87                                                                              & 50.00                                                                               & 100.00                                                                             \\ \hline
\end{tabular}
\caption{\label{tab:clustering_no_distance_50}Results of clustering mutants without Levensthein distance and cluster size n*0.50}
\end{table*}


Figure~\ref{box:clustering_no_distance_50} displays the box-plots of the weighted mutation score obtained from each individual sample. 
We observe that for every box-plot the \textit{p-value} is below 0.05 or 5\%.
Table~\ref{tab:clustering_no_distance_50} displays the results of clustering mutants without Levensthein distance and the number of clusters equal to the total amount of mutants * 0.50.
The maximum and minimum differences between the score of a full set and clustered set are 3.65\%, and 0.03\%, respectively.
We observe that the average cluster accuracy is higher than the average cluster accuracy of the 25\% set. 
The average of the differences between full set score and clustered score is 0.49\% which is smaller than the average difference than that of the 25\% set.

\begin{figure}[ht]
\includegraphics[width=0.5 \textwidth]{images/boxplot_summary/boxplot_hc_no_distance_0.75.png}
\caption{\label{box:clustering_no_distance_75}Box-plots containing weighted mutation score of clustering mutants without Levensthein distance and cluster size n*0.75}
\end{figure}

\begin{table*}[ht]
\centering
\begin{tabular}{|l|c|c|c|c|c|}
\hline
\multicolumn{1}{|c|}{Project} & \begin{tabular}[c]{@{}c@{}}ms\\ full set\end{tabular} & \begin{tabular}[c]{@{}c@{}}ms \\ clustered set\end{tabular} & \begin{tabular}[c]{@{}c@{}}Avg. ca\end{tabular} & \begin{tabular}[c]{@{}c@{}}Min. ca\end{tabular} & \begin{tabular}[c]{@{}c@{}}ca\end{tabular} \\ \hline
Google Auto Service           & 0.00                                                                                  & 0.00                                                                                      & 0.00                                                                               & 0.00                                                                                & 0.00                                                                               \\ \hline
ScribeJava-Core               & 33.52                                                                                 & 33.48                                                                                     & 98.63                                                                              & 50.00                                                                               & 100.00                                                                             \\ \hline
Google Auto Factory           & 48.41                                                                                 & 48.34                                                                                     & 98.41                                                                              & 50.00                                                                               & 100.00                                                                             \\ \hline
Google Auto Common            & 59.59                                                                                 & 59.71                                                                                     & 97.08                                                                              & 50.00                                                                               & 100.00                                                                             \\ \hline
Google Auto Value             & 61.42                                                                                 & 62.66                                                                                     & 97.99                                                                              & 50.00                                                                               & 100.00                                                                             \\ \hline
Google Gson                   & 66.83                                                                                 & 66.84                                                                                     & 97.37                                                                              & 50.00                                                                               & 100.00                                                                             \\ \hline
commons-io                    & 69.70                                                                                 & 66.07                                                                                     & 97.34                                                                              & 50.00                                                                               & 100.00                                                                             \\ \hline
commons-cli                   & 79.07                                                                                 & 79.12                                                                                     & 96.83                                                                              & 50.00                                                                               & 100.00                                                                             \\ \hline
commons-text                  & 79.29                                                                                 & 79.33                                                                                     & 97.60                                                                              & 50.00                                                                               & 100.00                                                                             \\ \hline
commons-codec                 & 84.56                                                                                 & 84.61                                                                                     & 98.43                                                                              & 50.00                                                                               & 100.00                                                                             \\ \hline
commons-csv                   & 85.16                                                                                 & 85.07                                                                                     & 97.49                                                                              & 50.00                                                                               & 100.00                                                                             \\ \hline
\end{tabular}
\caption{\label{tab:clustering_no_distance_75}Results of clustering mutant without Levensthein distance and cluster size n*0.75}
\end{table*}

Figure~\ref{box:clustering_no_distance_75} displays the box-plots of the weighted mutation score obtained from each individual sample. 
We observe that for every box-plot the \textit{p-value} is below 0.05 or 5\%.
Table~\ref{tab:clustering_no_distance_75} displays the results of clustering mutants without Levensthein distance and the number of clusters equal to the total amount of mutants * 0.50.
The maximum and minimum differences between the score of a full set and clustered set are 3.63\%, and 0.04\%, respectively.
We observe that the average accuracy is higher than the average accuracy of the other sets.
The average of the differences between full set score and clustered score is  0.49\% which is higher than the average difference than that of the 25\% and the same of that of the 50\% set.

\subsection{Fuzzy c-means}
\label{ch:results_rq2}
We present the results of our second experiment.
We display the weighted mutation score to create box-plots.
Similarly to Section~\ref{ch:results}, we removed the Levenshtein distance characteristic.

The data presented is the average of the results of the 30 repetitions of the experiment.
The result of each single repetition can be found in our Github repository in the folder \textit{experiment{\_}results}\cite{rbasarat-repo}.
The fuzzy c-means model requires the amount of clusters as a parameter.
Instead of displaying the reduction amount, as we did in the results of experiment one, we display the number of clusters that is calculated by the subtractive clustering model(See section \ref{ch:algorithm_parameters}.

\begin{figure}[ht]
\includegraphics[width=0.5 \textwidth]{images/fcm_summary/fcm_no_distance.png}
\caption{\label{box:clustering_fcm_no_distance_25}Box-plots containing weighted mutation score of clustering mutants without Levenshtein distance characteristic.}
\end{figure}

\begin{table*}[ht]
\centering
\begin{tabular}{|l|l|l|l|l|l|l|}
\hline
Project               & \begin{tabular}[c]{@{}l@{}}No. clusters\end{tabular} & \begin{tabular}[c]{@{}l@{}}Ms full set\end{tabular} & \begin{tabular}[c]{@{}l@{}}Ms cluster- \\ed set\end{tabular} & \begin{tabular}[c]{@{}l@{}}Avg ca\end{tabular} & \begin{tabular}[c]{@{}l@{}}Min.  ca\end{tabular} & \begin{tabular}[c]{@{}l@{}}Max. ca\end{tabular} \\ \hline
Google Auto   Service & 17                   & 0.00                            & 0.00                                                 & 0.00                             & 0.00                                                                                                                                     & 0.00                                                                                                                                     \\ \hline
ScribeJava-Core       & 32                   & 33.52                           & 23.64                                                & 76.80                            & 58.77                                                                                                                                    & 96.60                                                                                                                                    \\ \hline
Google Auto   Factory & 22                   & 48.41                           & 57.40                                                & 59.20                            & 50.67                                                                                                                                    & 73.96                                                                                                                                    \\ \hline
Google Auto   Common  & 31                   & 59.59                           & 37.64                                                & 62.95                            & 50.59                                                                                                                                    & 83.53                                                                                                                                    \\ \hline
Google Auto   Value   & 41                   & 61.42                           & 59.61                                                & 61.48                            & 50.60                                                                                                                                    & 83.53                                                                                                                                    \\ \hline
Google Gson           & 40                   & 66.83                           & 57.61                                                & 59.11                            & 50.84                                                                                                                                    & 76.68                                                                                                                                    \\ \hline
Commons-io            & 40                   & 69.70                           & 57.95                                                & 57.25                            & 50.62                                                                                                                                    & 72.70                                                                                                                                    \\ \hline
Commons-cli           & 26                   & 79.07                           & 81.53                                                & 81.77                            & 67.70                                                                                                                                    & 95.75                                                                                                                                    \\ \hline
Commons-text          & 55                   & 79.29                           & 77.92                                                & 78.22                            & 61.80                                                                                                                                    & 93.92                                                                                                                                    \\ \hline
Commons-codec         & 17                   & 84.56                           & 79.46                                                & 77.73                            & 74.39                                                                                                                                    & 81.66                                                                                                                                    \\ \hline
Commons-csv           & 32                   & 85.16                           & 82.84                                                & 83.47                            & 65.58                                                                                                                                    & 97.80                                                                                                                                    \\ \hline
Commons-lang          & 28                   & 78.31                           & 76.83                                                & 76.99                            & 73.03                                                                                                                                    & 80.56                                                                                                                                    \\ \hline
Jfreechart            & 28                   & 27.02                           & 26.41                                                & 74.11                            & 71.83                                                                                                                                    & 76.36                                                                                                                                    \\ \hline
Jodatime              & 25                   & 70.30                           & 65.17                                                & 64.75                            & 60.82                                                                                                                                    & 69.81                                                                                                                                    \\ \hline
Zxing                 & 28                   & 70.64                           & 80.03                                                & 83.11                            & 80.10                                                                                                                                    & 86.03                                                                                                                                    \\ \hline
\end{tabular}
\caption{\label{tab:ml_no_distance}Results of clustering mutants with the fuzzy c-means model without Levenshtein distance}
\end{table*}

Figure~\ref{box:clustering_fcm_no_distance_25} displays the box-plots of the weighted mutation score obtained from each individual sample. 
We observe that for every box-plot the \textit{p-value} is more than 0.05 or 5\%.
Table~\ref{tab:ml_no_distance} displays the results of clustering mutants without Levenshtein distance.
The maximum and minimum differences between the score of a full set and clustered set are 21.95\%, and 0.61\%, respectively.
The average of the differences between full set score and clustered score is 4.44\% which is bigger than the average difference than that of the results with all characteristics.

\section{Discussion}
The maximum deviation from the original mutation score of the experiment with hierarchical clustering, is below the statistical threshold.
We observe that the distances between the whiskers of the box-plots do not stretch more than the \textit{alpha-value}=0.05.
This means that there is not one sample that deviated more than 5\% from the original mutation score.
Thus we can reject the null hypothesis and accept our alternative hypothesis.

As hypothesised we observe that it is possible to cluster mutants with the set of characteristics we identified while maintaining effectiveness and reducing the amount executed. The hypothesis holds for all the projects and variants of the experiment: 66\% of the displayed results have a deviation of less than 1\%. 
This includes both variants of the experiment.
The results show that the set of characteristics can be used to cluster mutants efficiently.
A higher cluster accuracy means a score closer to that of the original.
If all clusters had an accuracy of 100\% we will achieve the same mutation score as that of the full set.
However we do not observe this behaviour in our results.
This is due to our random sampling method.
If we would select a mutant in the majority of each cluster the accuracy per cluster would correlate to the deviation in mutation score.
Unfortunately we do not know the majority of a cluster as we do not know if a mutant survives or is killed before execution.
If we want to find out we need to execute every mutant that is generated which equals running a full suite.
This defeats our purpose of increasing the performance by executing less mutants.

The maximum deviation from the original mutation score of the experiment with the fuzzy c-means model, is above the statistical threshold.
We observe that the distance between whiskers of the box-plots is more than the \textit{alpha-value} of 0.05.
Thus we cannot reject the null hypothesis.

The biggest deviation measured is 22.39\%.
This is significantly higher than the biggest deviation measured in the first experiment. 56\% of the results of both variants had a deviation in mutation score bigger than 5\%. Our hypothesis remains inconclusive for clustering the mutants with the fuzzy c-means model.

We observe that the number of clusters does not have an obvious relation to the total amount of mutants. 
This is expected as the subtractive clustering does not look at the total amount of data points(mutants) but to the contents(characteristics) of the data points.
The number of clusters compared to the that of the hierarchical clustering is significantly lower.
We did not research the exact clusters centroids, due to a limitation of resources.
The subtractive model calculated a low number of clusters.
This may indicate a close relationships between certain mutants, more research is needed.

We observe that the minimum lowest minimum cluster accuracy measured over all variants and projects is 50.54\% with and average of 62\%. 
This is higher than the minimum of the hierarchical clustering.
This means that in all project there was not one cluster that was evenly divided.
While the minimum accuracy was higher, the maximum accuracy was lower compared to the maximum accuracy of the hierarchical clustering results.
The highest accuracy measured over all variants and projects is 97.80\% with an average of 83.79\%
This means that in all project there was not one cluster that consisted of only survived or killed mutants.
This is caused by the relatively low number of clusters.
One cluster contains more mutants which results in a bigger weight per cluster.
\section{Related work}
\label{ch:related_work}
Clustering mutants has been done before.
The research that exists tries to cluster mutants by defining the centroid of a cluster and cluster mutants on that definition.
Our research extends mutation clustering by defining a centroid that is dynamic.
In other words the centroids of our clusters are defined by the mutants itself.

\subsection{Clustering overlapped mutants}
\label{ch:overlapping_mutants}
Ma et al., clustered mutants by overlap\cite{Ma2016}.
They defined the term overlap as as mutants that are functionally equivalent.
This is close to an equivalent mutant but not the same.
They explain that an equivalent mutants is functionally identical to the original source code while an overlapped mutant is functionally identical to at least one other mutant.
If a mutant does not have an overlapped mutant that mutant would be a cluster on its own.
The achieved a reduction, in mutants executed, of 10\%.
They write in their research that it is limited to the mutator operators on expression level. 
If a mutator did not generate two or more mutants they could not detect overlapping mutants.
Their future work includes adding more mutators and widening the scope level to statements or blocks.
\newline
The research does not include efficiency.
While they show that they can cluster mutants based on overlap they do not show how efficient the mutation testing process is compared to executing a full set of mutants.
A threat to their validity, which is not addressed, is the fact that two overlapping mutants may not be challenged by the same tests and thus can result in different results.
Our research solves this problem by taking into account the location of the mutants.

\subsection{Clustering by scope}
Yu et al., extended the research about overlapping mutants(see section \ref{ch:overlapping_mutants})~\cite{Yu2019PossibilityScope}.
They extended the scope by adding more mutators.
The also extended the research to include statements and blocks.
Their results shows an increase in clustered mutants.
The research of Yu et al., suffers from the same limitations and threats  mentioned in Chapter \ref{ch:overlapping_mutants}.

\subsection{Clustering Hamming distance}
Ji et al., did a qualitative domain analysis on mutants\cite{Ji2009}.
As a result they identified the Hamming distance to cluster with. 
The Hamming distance is, like the Levenshtein distance, a similarity measure.
They use the k-means algorithm to cluster mutants represented by Hamming distance.
They write that the reduced test set in their experiment is still as strong as the original test set\cite{Ji2009}.
\newline
Ji et al., were successful in clustering mutants with Hamming distance and the k-means algorithm.
They do acknowledge that their research still has problems with rationally determining the domains of their clusters.
Our research does not have this problem, as the centre of the domains are decided by the characteristics of the mutants.

\subsection{Spectral clustering}
Wei et al., makes use of an intelligent technique, namely spectral clustering, to improve the efficacy of mutant reduction\cite{Wei2021SpectralTesting}.
They defined multiple definitions for the mutants; similarity distance, distance between the mutants and a killing matrix of the mutants.
With these definitions they reduced the mutants based on their proposed reduction method.
This method is based on spectral clustering (SCMT), the determination method of the number of clusters, spectral clustering of mutants, and selection of representative mutants.
The reduced mutants were then clustered with a classical clustering algorithm.
Their results are promising and show high cluster accuracy.
They write that there is still work left to do in optimizing the matrices and clustering algorithms.
Our goal overlaps with that of Wei, the difference is that we used a different methodology to achieve that goal.
Another difference is that our research does not require to know the result of a mutant(killed or survived).

\subsection{Clustering similarity}
Hussain et al., used the k-means and agglomerative clustering algorithm to cluster mutants according to a similarity measure. 
They used the Hamming distance as similarity measure.
They calculated the distance and used this as data to feed into the clustering algorithms.
The number of clusters and the initial position of the cluster center in the k-means algorithm are difficult to determine, and the process of the agglomerative clustering algorithm makes it difficult to correct the existing cluster formation\cite{Hussain2008}.
They did take into account efficiency.
Hussain et al, had the same goals as our research. 
The difference is that our research used different and more elaborated methods to cluster mutants.
Hussain et al., calculated the mutation score by counting one cluster as one mutant.
Our research calculated a weighted mutation score to reflect the clusters more accurately.

\subsection{Generalizing mutants}
Wilinski et al., tried to generalize the mutants by defining metrics.
Each metric is calculated for a specific mutator.
The three metrics are usefulness, frequency and dependency.
Combining the results of usefulness and frequency metrics, they observed that reducing the number of generated mutants gives noticeable cost reduction without a loss of the mutation score accuracy.
Wilinski et al., their research is narrowly scoped to the specific mutators they decided to do research on.
Our research does not contain this limitation.
The mutator is used as a characteristic in our research instead of limiting our research it helps define a mutant.

\section{Conclusion}
\label{ch:conclusion}
The high temporal cost requirement is often a barrier for adopting mutation testing~\cite{Pizzoleto2019}. An important technique to reduce the number of mutants, and thus the number of test case executions is mutation clustering~\cite{Ma2016,Yu2019PossibilityScope}, which has been researched with promising results~\cite{Ji2009,Wilinski2015,Ma2016}. 
However, the reduction in temporal cost is still insufficient when considering CI/CD pipelines, and the cost outweighs the benefits.

To reduce the cost of mutation testing, we aim to find a solution that can cluster mutants while maintaining the accuracy of the same complete set of mutants. 

While preparing our research, we evaluated three mutation testing tools and found that two out of three mutation testing tools were not actively maintained anymore. The mutation testing tool PIT scored best on the metrics measured in our replication study, this is also the tool we decided to use for our research.

We performed a qualitative analysis on mutant characteristics.
As a result we created a list of identified characteristics based on existing research. We then extended this list by identifying characteristic with logic and data available.
We used the identified characteristics to represent the mutants.

We clustered the mutants with a white box approach and a black box approach. For our white box approach we selected the hierarchical clustering algorithm. The hierarchical clustering showed that we can cluster mutants while maintaining effectiveness.
We achieved reductions of up to 75\% of the total amount of mutants without significantly reducing the accuracy.

For the black box approach we selected a machine learning model, specifically the fuzzy c-means model. The fuzzy c-means model showed a significant reduction in effectiveness. We trained the fuzzy c-means model with parameters that were optimal according to existing research. 
We used a different machine learning model, specialized in calculating the number of clusters for fuzzy clustering, to calculate the number of clusters. While the minimum clustering accuracy was higher than that of the hierarchical clustering, the maximum cluster accuracy was lower than that of the hierarchical clustering. The cluster accuracy had a larger impact on the clusters of the machine learning model because there was a relatively small number of clusters.

Our research shows that it is possible to determine the means of domains as the centre of a cluster within a programming language without narrowing down the scope to specific pieces of code. The centroids our research identified are decided by the characteristics of the mutants.
It can be applied to all Java projects that can be run with PIT, which is a larger scope than in existing mutation clustering research.

\printbibliography[heading=bibintoc]

\end{document}
